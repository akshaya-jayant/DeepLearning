{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>IST 597 Foundations of Deep Learning</center></h1>\n",
        "\n",
        "---\n",
        "\n",
        "<h2><center>Assignment 10</center><h2>\n",
        "\n",
        "**Instructor** : Dr. C. Lee Giles <br>\n",
        "**TA** : Neisarg Dave, Shaurya Rohatgi\n",
        "<br><br>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WgzYdyhY4_mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><center>Multi Layer Perceptron</center><h2>\n",
        "\n",
        "Multiplayer Perceptron is a feed forward neural network composed of multiple layers of affine transformations followed by non linearity function. Each layer of an MLP is defined as:\n",
        "$$\n",
        "z^{(l)} = W^{(l)}x^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "x^{(l)} = f(z^{(l)})\n",
        "$$\n",
        "\n",
        "Here, $l$ denotes the layer of MLP, $W$ and $b$ are the trainable parameters called *weight* and *bias* respectively\n",
        "\n",
        "ReLU or Rectified Linear Unit is the most common non linear layer used with MLP.\n",
        "More about ReLU can be found here: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"
      ],
      "metadata": {
        "id": "40nCWOmj7Xjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you are required to do the following:\n",
        "\n",
        "+ Create an MLP\n",
        "  + Write the *forward* function for affine transformation\n",
        "  + Write the *backward* function (to calculate gradients) for the affine transformation\n",
        "  + Create a Neural Network by stacking these transformations\n",
        "\n",
        "+ Train the Neural Network on Fashion MNIST Dataset\n",
        "  + Write Loss Function for the training\n",
        "  + Write parameter update routine\n",
        "  + Run Validation\n",
        "  + Plot Train and Validation Loss Curves using Matplotlib\n",
        "\n",
        "+ Testing the Neural Network\n",
        "  + Change the *seed* and train the neural network 10 times\n",
        "  + Report the mean and variance over 10 trials on the following metrics:\n",
        "    + Accuracy\n",
        "    + Precision\n",
        "    + Recall\n",
        "    + F1\n",
        "    + ROC curve\n",
        "\n",
        "+ Change the number of parameters in the Neural Network\n",
        "  + Train a underfit model\n",
        "  + Train a overfit model\n",
        "  + Demostrate the difference between overfitting and underfitting\n"
      ],
      "metadata": {
        "id": "ekbdxA4KtmSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# select device as cuda\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# set random seeds\n",
        "seed = 1234 ## change this seed when you run trials\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJYSA_Cm1fnV",
        "outputId": "85523836-e811-4a70-c644-507713f0e93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1d04fb93d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Update\n",
        "To update gradients, from the equation of affine transformation we have:\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)}x^{(l-1)} + b\n",
        "$$\n",
        "\n",
        "We can calulate all the partial derivations in the following fashion\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z^{(l)}}{\\partial x^{(l-1)}} = W^{(l)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z^{(l)}}{\\partial W^{(l)}} = x^{(l-1)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z^{(l)}}{\\partial b^{(l)}} = 1\n",
        "$$\n",
        "\n",
        "To write the *backward* function, we need one more step. *backward* functon takes *grad_output* as its input. It is the backward flow of gradients coming from the final output $\\hat y$ and can be mathematically expressed as:\n",
        "$$\n",
        "  grad\\_output = \\frac{\\partial \\hat y}{\\partial z^{(l)}}\n",
        "$$\n",
        "\n",
        "*backward* functions gives out three values:\n",
        "\n",
        "$$\n",
        "grad\\_x = \\frac{\\partial \\hat y}{\\partial x^{(l-1)}} = \\frac{\\partial \\hat y}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial x^{(l-1)}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "grad\\_w = \\frac{\\partial \\hat y}{\\partial W^{(l)}} = \\frac{\\partial \\hat y}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "grad\\_b = \\frac{\\partial \\hat y}{\\partial b^{(l)}} =  \\frac{\\partial \\hat y}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial b^{(l)}}\n",
        "$$\n",
        "\n",
        "In the code below, update the *forward* and *backward* functions of class LinearFunction\n"
      ],
      "metadata": {
        "id": "CVBLiKBUxK68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearFunction(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, W, b):\n",
        "    \"\"\"\n",
        "    # x -> input matrix of size n_samples x sdim\n",
        "    # W -> transformation matrix\n",
        "    # b -> bias term\n",
        "    \"\"\"\n",
        "    ctx.save_for_backward(x, W, b)\n",
        "    # Affine transformation:\n",
        "    #-------------------\n",
        "    # z = x. W + b\n",
        "    z = torch.matmul(x, W) + b\n",
        "    #-------------------\n",
        "    return z\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    x, W, b = ctx.saved_tensors\n",
        "\n",
        "    # Gradient updates:\n",
        "    #----------------------------\n",
        "    grad_x = torch.matmul(grad_output,W.T)\n",
        "    grad_w = torch.matmul(grad_output.T,x).T\n",
        "    grad_b = grad_output\n",
        "    #-----------------------------\n",
        "    return grad_x, grad_w, grad_b\n"
      ],
      "metadata": {
        "id": "pcgiaXw35f71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class CustomLinearLayer uses previously defined LinearFunction to create one layer of Neural Network. Look at the initial values given to $W$ and $b$.\n",
        "You do not need to change anything in this class."
      ],
      "metadata": {
        "id": "f0-GkEJs19ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLinearLayer(torch.nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super(CustomLinearLayer, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    w = torch.normal(mean = 0, std = 0.1, size = [in_features, out_features], requires_grad=True)\n",
        "    b = torch.full([out_features], 0.01, requires_grad=True)\n",
        "    w = torch.nn.Parameter(w)\n",
        "    b = torch.nn.Parameter(b)\n",
        "    self.register_parameter('w', w)\n",
        "    self.register_parameter('b', b)\n",
        "    self.linear_function = LinearFunction.apply\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_function(x, self.w, self.b)"
      ],
      "metadata": {
        "id": "ksWyV7mSK15f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Neural Network class below class Below, Add two linear layers of the following sizes:\n",
        "+ 784 x 512\n",
        "+ 512 x 10\n",
        "\n",
        "In the forward function,\n",
        "+ Apply layer_1 on input\n",
        "+ Apply activation function on the result\n",
        "+ Apply layer_2 on the result\n",
        "+ Apply Softmax and return the final result"
      ],
      "metadata": {
        "id": "rXWwi0a02frQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    # Add Linear layers below with the given dimensions\n",
        "    #-----------------------------------\n",
        "    self.layer_1 = CustomLinearLayer(784, 512)\n",
        "    self.layer_2 = CustomLinearLayer(512, 10)\n",
        "    # ------------------------------------\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Applying layers defined above:\n",
        "    #-------------------------------\n",
        "    output_1 = self.layer_1(x)\n",
        "    output_1_actv = self.activation(output_1)\n",
        "    output_2 = self.layer_2(output_1_actv)\n",
        "    output = self.softmax(output_2)\n",
        "    #-------------------------------\n",
        "    return output"
      ],
      "metadata": {
        "id": "E5M3gwT7LDLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fasion MNIST dataset is a set of grayscale images categorized into 10 classes. To train our neural network, first we need to define loss function. $y$ is the ground truth class label and $probs$ is the probability distribution given by our Neural Network over 10 classes, update the *forward* function of loss class below.\n",
        "\n",
        "$$\n",
        "ce\\_loss = -\\frac{1}{{\\#samples}}\\sum^{\\#samples}_i y_i.\\log(probs_i)\n",
        "$$\n",
        "\n",
        "*Hint*: Try representing $y$ as one hot vector"
      ],
      "metadata": {
        "id": "pH5-K2PP33lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CrossEntropyLoss, self).__init__()\n",
        "\n",
        "  def forward(self, probs, y):\n",
        "    #Loss function here:\n",
        "    #-------------------------------\n",
        "    loss = -1 * torch.mean(torch.sum(torch.nn.functional.one_hot(y, num_classes= 10) * torch.log(probs),axis=1))\n",
        "    #-------------------------------\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Mme5RblpOpYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "9OvSv0bazhlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "EleFGZ8W4wGM",
        "outputId": "0d68eaab-5f30-4965-eed1-04f830e8aff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion MNIST dataset loaded from pytorch APIs only has two sets: train and test. Split the train data into train and validation sets with 50K samples in train and 10K samples in validation set. The data split should be random.\n",
        "\n",
        "*Hint* Look for a function in *torch.utils.data*"
      ],
      "metadata": {
        "id": "sZ90GGxc58X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split random mnist trainset into two sets: mnist trainset -> 50000 , mnist validation set -> 10000\n",
        "#----------------\n",
        "train_data, val_data = torch.utils.data.random_split(training_data,[50000, 10000])\n",
        "#----------------"
      ],
      "metadata": {
        "id": "BachxlStz4E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSeXx9qL66UU",
        "outputId": "787fc3bc-0750-47c3-b596-1f1d258b6ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().cuda()\n",
        "ce_loss = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "LbhN71ej1Ry8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) The gradients we calculated in the start can be accessed by iterating over model.parameters() as *params.grad*. Write the parameter update rule in the training code below:\n",
        "\n",
        "$$\n",
        "param := param - learning\\_rate*gradient\n",
        "$$\n",
        "\n",
        "2) Write validation loop which runs every epoch and prints the following metrics on validation set:\n",
        "  + Accuracy\n",
        "  + Precision\n",
        "  + Recall\n",
        "  + F1\n",
        "  + roc_auc\n",
        "\n",
        "3) Store the training loss and validation loss in each iteration and plot a graph comparing them."
      ],
      "metadata": {
        "id": "Qh0sY9487KoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report,roc_auc_score,f1_score,precision_score,recall_score\n",
        "\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "valid_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "max_epochs = 10\n",
        "learning_rate = 0.01\n",
        "train_iter_loss = []\n",
        "valid_iter_loss = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  for idx, data in enumerate(train_loader):\n",
        "    features, labels = data\n",
        "\n",
        "    features = features.cuda()\n",
        "\n",
        "    labels = labels.cuda()\n",
        "    probs = model(features.reshape([-1, 784]))\n",
        "\n",
        "    loss = ce_loss(probs, labels)\n",
        "    train_iter_loss.append(loss.item())\n",
        "    print(\"Epoch {0}/{1} Iteration {2}/{3} Loss {4}: \".format(epoch, max_epochs, idx, len(train_loader), loss))\n",
        "\n",
        "    for param in model.parameters():\n",
        "      param.grad = None\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      # Parameter update routine:\n",
        "      # --------------\n",
        "      new_param = param - learning_rate * param.grad\n",
        "\n",
        "      # --------------\n",
        "      with torch.no_grad():\n",
        "        param.copy_(new_param)\n",
        "  # Taking the mean of training loss in each iteration\n",
        "  training_loss.append(torch.mean(torch.Tensor(train_iter_loss)))\n",
        "\n",
        "  # Variable initialization for each epoch\n",
        "  total = 0\n",
        "  accuracy_total = 0\n",
        "  recall_total = []\n",
        "  precision_total = []\n",
        "  f1_total = []\n",
        "  roc_auc_total = []\n",
        "\n",
        "  # Validation routine here:\n",
        "  #----------------------------\n",
        "  for idx, valid_data in enumerate(valid_loader):\n",
        "    features, labels = valid_data\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      probs = model(features.reshape([-1, 784]))\n",
        "      max_prob  = torch.argmax(probs, dim=1)\n",
        "      loss = ce_loss(probs, labels)\n",
        "      # Validation loss in each iteration will be whole iteration\n",
        "      valid_iter_loss.append(loss.item())\n",
        "\n",
        "      # The expected class for the given input\n",
        "      y_true = labels.cpu().numpy()\n",
        "      # The predicted class for the given input\n",
        "      y_pred = probs.argmax(axis=1).cpu().numpy()\n",
        "      # The predicted class score for the given input\n",
        "      y_score = probs.cpu().detach().numpy()\n",
        "\n",
        "      # Calculation of the metrics for the validation set\n",
        "      precision = precision_score(y_true, y_pred,average ='macro')\n",
        "      recall = recall_score(y_true, y_pred, average ='macro')\n",
        "      f1 = f1_score(y_true, y_pred,average = 'macro')\n",
        "      roc_auc = roc_auc_score(y_true, y_score,multi_class='ovr')\n",
        "\n",
        "      accuracy_total+= sum(labels==max_prob).item()\n",
        "      total +=labels.shape[0]\n",
        "\n",
        "      recall_total.append(recall)\n",
        "      precision_total.append(precision)\n",
        "      f1_total.append(f1)\n",
        "      roc_auc_total.append(roc_auc)\n",
        "\n",
        "  validation_loss.append(torch.mean(torch.Tensor(valid_iter_loss)))\n",
        "  accuracy = accuracy_total/total\n",
        "  recall_mean = sum(recall_total)/len(recall_total)\n",
        "  precision_mean = sum(precision_total)/len(precision_total)\n",
        "  f1_mean = sum(f1_total)/len(f1_total)\n",
        "  roc_auc_mean = sum(roc_auc_total)/len(roc_auc_total)\n",
        "\n",
        "  print(\"Validation Epoch {0}/{1} Iteration {2}/{3} accuracy {4}: \".format(epoch, max_epochs, idx, len(valid_loader), accuracy))\n",
        "  print(\"Validation Epoch {0}/{1} Iteration {2}/{3} recall_mean {4}: \".format(epoch, max_epochs, idx, len(valid_loader), recall_mean))\n",
        "  print(\"Validation Epoch {0}/{1} Iteration {2}/{3} precision_mean {4}: \".format(epoch, max_epochs, idx, len(valid_loader), precision_mean))\n",
        "  print(\"Validation Epoch {0}/{1} Iteration {2}/{3} f1_mean {4}: \".format(epoch, max_epochs, idx, len(valid_loader), f1_mean))\n",
        "  print(\"Validation Epoch {0}/{1} Iteration {2}/{3} roc_auc_mean {4}: \".format(epoch, max_epochs, idx, len(valid_loader), roc_auc_mean))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "plt.plot(training_loss, label='Training Loss')\n",
        "plt.plot(validation_loss, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uNL16slmMTST",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae20fdd2-8359-4f50-d221-ab1101f4f65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10 Iteration 0/49 Loss 3.6421902179718018: \n",
            "Epoch 0/10 Iteration 1/49 Loss 2.8335654735565186: \n",
            "Epoch 0/10 Iteration 2/49 Loss 2.489055633544922: \n",
            "Epoch 0/10 Iteration 3/49 Loss 2.36944580078125: \n",
            "Epoch 0/10 Iteration 4/49 Loss 2.2263412475585938: \n",
            "Epoch 0/10 Iteration 5/49 Loss 2.039961814880371: \n",
            "Epoch 0/10 Iteration 6/49 Loss 1.96735417842865: \n",
            "Epoch 0/10 Iteration 7/49 Loss 1.9270508289337158: \n",
            "Epoch 0/10 Iteration 8/49 Loss 1.909238338470459: \n",
            "Epoch 0/10 Iteration 9/49 Loss 1.829696536064148: \n",
            "Epoch 0/10 Iteration 10/49 Loss 1.736072301864624: \n",
            "Epoch 0/10 Iteration 11/49 Loss 1.6891040802001953: \n",
            "Epoch 0/10 Iteration 12/49 Loss 1.6831845045089722: \n",
            "Epoch 0/10 Iteration 13/49 Loss 1.6354825496673584: \n",
            "Epoch 0/10 Iteration 14/49 Loss 1.6609686613082886: \n",
            "Epoch 0/10 Iteration 15/49 Loss 1.546505331993103: \n",
            "Epoch 0/10 Iteration 16/49 Loss 1.5746090412139893: \n",
            "Epoch 0/10 Iteration 17/49 Loss 1.5222102403640747: \n",
            "Epoch 0/10 Iteration 18/49 Loss 1.5270293951034546: \n",
            "Epoch 0/10 Iteration 19/49 Loss 1.5160634517669678: \n",
            "Epoch 0/10 Iteration 20/49 Loss 1.4762182235717773: \n",
            "Epoch 0/10 Iteration 21/49 Loss 1.4277839660644531: \n",
            "Epoch 0/10 Iteration 22/49 Loss 1.420271873474121: \n",
            "Epoch 0/10 Iteration 23/49 Loss 1.3812944889068604: \n",
            "Epoch 0/10 Iteration 24/49 Loss 1.4099159240722656: \n",
            "Epoch 0/10 Iteration 25/49 Loss 1.3824234008789062: \n",
            "Epoch 0/10 Iteration 26/49 Loss 1.329432725906372: \n",
            "Epoch 0/10 Iteration 27/49 Loss 1.3598614931106567: \n",
            "Epoch 0/10 Iteration 28/49 Loss 1.2869887351989746: \n",
            "Epoch 0/10 Iteration 29/49 Loss 1.3237316608428955: \n",
            "Epoch 0/10 Iteration 30/49 Loss 1.2886197566986084: \n",
            "Epoch 0/10 Iteration 31/49 Loss 1.2482260465621948: \n",
            "Epoch 0/10 Iteration 32/49 Loss 1.2516343593597412: \n",
            "Epoch 0/10 Iteration 33/49 Loss 1.2749042510986328: \n",
            "Epoch 0/10 Iteration 34/49 Loss 1.2298707962036133: \n",
            "Epoch 0/10 Iteration 35/49 Loss 1.199073314666748: \n",
            "Epoch 0/10 Iteration 36/49 Loss 1.175027847290039: \n",
            "Epoch 0/10 Iteration 37/49 Loss 1.1827633380889893: \n",
            "Epoch 0/10 Iteration 38/49 Loss 1.2051677703857422: \n",
            "Epoch 0/10 Iteration 39/49 Loss 1.2086211442947388: \n",
            "Epoch 0/10 Iteration 40/49 Loss 1.1417934894561768: \n",
            "Epoch 0/10 Iteration 41/49 Loss 1.1067324876785278: \n",
            "Epoch 0/10 Iteration 42/49 Loss 1.149991512298584: \n",
            "Epoch 0/10 Iteration 43/49 Loss 1.1410813331604004: \n",
            "Epoch 0/10 Iteration 44/49 Loss 1.1514229774475098: \n",
            "Epoch 0/10 Iteration 45/49 Loss 1.1634063720703125: \n",
            "Epoch 0/10 Iteration 46/49 Loss 1.1595313549041748: \n",
            "Epoch 0/10 Iteration 47/49 Loss 1.100645899772644: \n",
            "Epoch 0/10 Iteration 48/49 Loss 1.1063220500946045: \n",
            "Validation Epoch 0/10 Iteration 9/10 accuracy 0.6303: \n",
            "Validation Epoch 0/10 Iteration 9/10 recall_mean 0.6304615975660919: \n",
            "Validation Epoch 0/10 Iteration 9/10 precision_mean 0.6250970417267192: \n",
            "Validation Epoch 0/10 Iteration 9/10 f1_mean 0.6176054645550161: \n",
            "Validation Epoch 0/10 Iteration 9/10 roc_auc_mean 0.9318310020804272: \n",
            "\n",
            "\n",
            "Epoch 1/10 Iteration 0/49 Loss 1.1173522472381592: \n",
            "Epoch 1/10 Iteration 1/49 Loss 1.103272795677185: \n",
            "Epoch 1/10 Iteration 2/49 Loss 1.0658588409423828: \n",
            "Epoch 1/10 Iteration 3/49 Loss 1.1049911975860596: \n",
            "Epoch 1/10 Iteration 4/49 Loss 1.0985673666000366: \n",
            "Epoch 1/10 Iteration 5/49 Loss 1.0585858821868896: \n",
            "Epoch 1/10 Iteration 6/49 Loss 1.0341492891311646: \n",
            "Epoch 1/10 Iteration 7/49 Loss 1.05790114402771: \n",
            "Epoch 1/10 Iteration 8/49 Loss 1.0976977348327637: \n",
            "Epoch 1/10 Iteration 9/49 Loss 1.0589702129364014: \n",
            "Epoch 1/10 Iteration 10/49 Loss 1.033712387084961: \n",
            "Epoch 1/10 Iteration 11/49 Loss 1.009735345840454: \n",
            "Epoch 1/10 Iteration 12/49 Loss 1.0213404893875122: \n",
            "Epoch 1/10 Iteration 13/49 Loss 1.0295381546020508: \n",
            "Epoch 1/10 Iteration 14/49 Loss 1.0396168231964111: \n",
            "Epoch 1/10 Iteration 15/49 Loss 0.9741941094398499: \n",
            "Epoch 1/10 Iteration 16/49 Loss 1.0375399589538574: \n",
            "Epoch 1/10 Iteration 17/49 Loss 0.9864515066146851: \n",
            "Epoch 1/10 Iteration 18/49 Loss 1.0284491777420044: \n",
            "Epoch 1/10 Iteration 19/49 Loss 1.0150599479675293: \n",
            "Epoch 1/10 Iteration 20/49 Loss 1.0077298879623413: \n",
            "Epoch 1/10 Iteration 21/49 Loss 0.9770614504814148: \n",
            "Epoch 1/10 Iteration 22/49 Loss 0.9917546510696411: \n",
            "Epoch 1/10 Iteration 23/49 Loss 0.9765269756317139: \n",
            "Epoch 1/10 Iteration 24/49 Loss 1.0302051305770874: \n",
            "Epoch 1/10 Iteration 25/49 Loss 0.9936473965644836: \n",
            "Epoch 1/10 Iteration 26/49 Loss 0.9624885320663452: \n",
            "Epoch 1/10 Iteration 27/49 Loss 0.9960004091262817: \n",
            "Epoch 1/10 Iteration 28/49 Loss 0.9393760561943054: \n",
            "Epoch 1/10 Iteration 29/49 Loss 1.00795316696167: \n",
            "Epoch 1/10 Iteration 30/49 Loss 0.9524348378181458: \n",
            "Epoch 1/10 Iteration 31/49 Loss 0.9314272403717041: \n",
            "Epoch 1/10 Iteration 32/49 Loss 0.9409328699111938: \n",
            "Epoch 1/10 Iteration 33/49 Loss 0.982864260673523: \n",
            "Epoch 1/10 Iteration 34/49 Loss 0.940855860710144: \n",
            "Epoch 1/10 Iteration 35/49 Loss 0.9097020626068115: \n",
            "Epoch 1/10 Iteration 36/49 Loss 0.9035211801528931: \n",
            "Epoch 1/10 Iteration 37/49 Loss 0.9344564080238342: \n",
            "Epoch 1/10 Iteration 38/49 Loss 0.9403944611549377: \n",
            "Epoch 1/10 Iteration 39/49 Loss 0.9423189163208008: \n",
            "Epoch 1/10 Iteration 40/49 Loss 0.89153653383255: \n",
            "Epoch 1/10 Iteration 41/49 Loss 0.8686894178390503: \n",
            "Epoch 1/10 Iteration 42/49 Loss 0.9084566831588745: \n",
            "Epoch 1/10 Iteration 43/49 Loss 0.9016744494438171: \n",
            "Epoch 1/10 Iteration 44/49 Loss 0.9238013029098511: \n",
            "Epoch 1/10 Iteration 45/49 Loss 0.9485490322113037: \n",
            "Epoch 1/10 Iteration 46/49 Loss 0.9490540623664856: \n",
            "Epoch 1/10 Iteration 47/49 Loss 0.8833065629005432: \n",
            "Epoch 1/10 Iteration 48/49 Loss 0.9059121608734131: \n",
            "Validation Epoch 1/10 Iteration 9/10 accuracy 0.6885: \n",
            "Validation Epoch 1/10 Iteration 9/10 recall_mean 0.6885340649824182: \n",
            "Validation Epoch 1/10 Iteration 9/10 precision_mean 0.6844119917844417: \n",
            "Validation Epoch 1/10 Iteration 9/10 f1_mean 0.680271148907344: \n",
            "Validation Epoch 1/10 Iteration 9/10 roc_auc_mean 0.9508988871859791: \n",
            "\n",
            "\n",
            "Epoch 2/10 Iteration 0/49 Loss 0.9197635054588318: \n",
            "Epoch 2/10 Iteration 1/49 Loss 0.9088121056556702: \n",
            "Epoch 2/10 Iteration 2/49 Loss 0.8732978701591492: \n",
            "Epoch 2/10 Iteration 3/49 Loss 0.9112831354141235: \n",
            "Epoch 2/10 Iteration 4/49 Loss 0.9177587032318115: \n",
            "Epoch 2/10 Iteration 5/49 Loss 0.8869551420211792: \n",
            "Epoch 2/10 Iteration 6/49 Loss 0.8642563819885254: \n",
            "Epoch 2/10 Iteration 7/49 Loss 0.8856199979782104: \n",
            "Epoch 2/10 Iteration 8/49 Loss 0.9211413860321045: \n",
            "Epoch 2/10 Iteration 9/49 Loss 0.8827550411224365: \n",
            "Epoch 2/10 Iteration 10/49 Loss 0.8779169321060181: \n",
            "Epoch 2/10 Iteration 11/49 Loss 0.8511692881584167: \n",
            "Epoch 2/10 Iteration 12/49 Loss 0.8616039752960205: \n",
            "Epoch 2/10 Iteration 13/49 Loss 0.8784072399139404: \n",
            "Epoch 2/10 Iteration 14/49 Loss 0.8749998211860657: \n",
            "Epoch 2/10 Iteration 15/49 Loss 0.8232506513595581: \n",
            "Epoch 2/10 Iteration 16/49 Loss 0.8913424015045166: \n",
            "Epoch 2/10 Iteration 17/49 Loss 0.8371481895446777: \n",
            "Epoch 2/10 Iteration 18/49 Loss 0.8812117576599121: \n",
            "Epoch 2/10 Iteration 19/49 Loss 0.8679277896881104: \n",
            "Epoch 2/10 Iteration 20/49 Loss 0.8690850734710693: \n",
            "Epoch 2/10 Iteration 21/49 Loss 0.8358721733093262: \n",
            "Epoch 2/10 Iteration 22/49 Loss 0.8594484329223633: \n",
            "Epoch 2/10 Iteration 23/49 Loss 0.8465906381607056: \n",
            "Epoch 2/10 Iteration 24/49 Loss 0.9070987701416016: \n",
            "Epoch 2/10 Iteration 25/49 Loss 0.862891674041748: \n",
            "Epoch 2/10 Iteration 26/49 Loss 0.8374965190887451: \n",
            "Epoch 2/10 Iteration 27/49 Loss 0.8687813878059387: \n",
            "Epoch 2/10 Iteration 28/49 Loss 0.8150563836097717: \n",
            "Epoch 2/10 Iteration 29/49 Loss 0.894569993019104: \n",
            "Epoch 2/10 Iteration 30/49 Loss 0.8300513029098511: \n",
            "Epoch 2/10 Iteration 31/49 Loss 0.8137917518615723: \n",
            "Epoch 2/10 Iteration 32/49 Loss 0.825652003288269: \n",
            "Epoch 2/10 Iteration 33/49 Loss 0.870010495185852: \n",
            "Epoch 2/10 Iteration 34/49 Loss 0.829350471496582: \n",
            "Epoch 2/10 Iteration 35/49 Loss 0.7978404760360718: \n",
            "Epoch 2/10 Iteration 36/49 Loss 0.7965620756149292: \n",
            "Epoch 2/10 Iteration 37/49 Loss 0.8327373266220093: \n",
            "Epoch 2/10 Iteration 38/49 Loss 0.8304023742675781: \n",
            "Epoch 2/10 Iteration 39/49 Loss 0.8292772769927979: \n",
            "Epoch 2/10 Iteration 40/49 Loss 0.7857128381729126: \n",
            "Epoch 2/10 Iteration 41/49 Loss 0.7682148218154907: \n",
            "Epoch 2/10 Iteration 42/49 Loss 0.8030646443367004: \n",
            "Epoch 2/10 Iteration 43/49 Loss 0.7976123094558716: \n",
            "Epoch 2/10 Iteration 44/49 Loss 0.8227312564849854: \n",
            "Epoch 2/10 Iteration 45/49 Loss 0.8511370420455933: \n",
            "Epoch 2/10 Iteration 46/49 Loss 0.8528294563293457: \n",
            "Epoch 2/10 Iteration 47/49 Loss 0.7834131121635437: \n",
            "Epoch 2/10 Iteration 48/49 Loss 0.8127210736274719: \n",
            "Validation Epoch 2/10 Iteration 9/10 accuracy 0.7216: \n",
            "Validation Epoch 2/10 Iteration 9/10 recall_mean 0.7219007652038083: \n",
            "Validation Epoch 2/10 Iteration 9/10 precision_mean 0.7178999111814832: \n",
            "Validation Epoch 2/10 Iteration 9/10 f1_mean 0.7160244202210486: \n",
            "Validation Epoch 2/10 Iteration 9/10 roc_auc_mean 0.9587702417027801: \n",
            "\n",
            "\n",
            "Epoch 3/10 Iteration 0/49 Loss 0.8275760412216187: \n",
            "Epoch 3/10 Iteration 1/49 Loss 0.8186608552932739: \n",
            "Epoch 3/10 Iteration 2/49 Loss 0.7829936742782593: \n",
            "Epoch 3/10 Iteration 3/49 Loss 0.8203024864196777: \n",
            "Epoch 3/10 Iteration 4/49 Loss 0.8294077515602112: \n",
            "Epoch 3/10 Iteration 5/49 Loss 0.8030941486358643: \n",
            "Epoch 3/10 Iteration 6/49 Loss 0.7816246747970581: \n",
            "Epoch 3/10 Iteration 7/49 Loss 0.8011853694915771: \n",
            "Epoch 3/10 Iteration 8/49 Loss 0.8302531242370605: \n",
            "Epoch 3/10 Iteration 9/49 Loss 0.7930243015289307: \n",
            "Epoch 3/10 Iteration 10/49 Loss 0.7998365163803101: \n",
            "Epoch 3/10 Iteration 11/49 Loss 0.7705177664756775: \n",
            "Epoch 3/10 Iteration 12/49 Loss 0.7794466018676758: \n",
            "Epoch 3/10 Iteration 13/49 Loss 0.7990950345993042: \n",
            "Epoch 3/10 Iteration 14/49 Loss 0.7900799512863159: \n",
            "Epoch 3/10 Iteration 15/49 Loss 0.7452924251556396: \n",
            "Epoch 3/10 Iteration 16/49 Loss 0.8142246603965759: \n",
            "Epoch 3/10 Iteration 17/49 Loss 0.758703887462616: \n",
            "Epoch 3/10 Iteration 18/49 Loss 0.8013191223144531: \n",
            "Epoch 3/10 Iteration 19/49 Loss 0.7895705103874207: \n",
            "Epoch 3/10 Iteration 20/49 Loss 0.7946641445159912: \n",
            "Epoch 3/10 Iteration 21/49 Loss 0.7583178281784058: \n",
            "Epoch 3/10 Iteration 22/49 Loss 0.78822922706604: \n",
            "Epoch 3/10 Iteration 23/49 Loss 0.7745354175567627: \n",
            "Epoch 3/10 Iteration 24/49 Loss 0.83901047706604: \n",
            "Epoch 3/10 Iteration 25/49 Loss 0.7896823883056641: \n",
            "Epoch 3/10 Iteration 26/49 Loss 0.7675412893295288: \n",
            "Epoch 3/10 Iteration 27/49 Loss 0.7970393896102905: \n",
            "Epoch 3/10 Iteration 28/49 Loss 0.7442444562911987: \n",
            "Epoch 3/10 Iteration 29/49 Loss 0.8287700414657593: \n",
            "Epoch 3/10 Iteration 30/49 Loss 0.7616159915924072: \n",
            "Epoch 3/10 Iteration 31/49 Loss 0.7459707260131836: \n",
            "Epoch 3/10 Iteration 32/49 Loss 0.7603310346603394: \n",
            "Epoch 3/10 Iteration 33/49 Loss 0.8041214942932129: \n",
            "Epoch 3/10 Iteration 34/49 Loss 0.7640331387519836: \n",
            "Epoch 3/10 Iteration 35/49 Loss 0.733473539352417: \n",
            "Epoch 3/10 Iteration 36/49 Loss 0.7336338758468628: \n",
            "Epoch 3/10 Iteration 37/49 Loss 0.7716083526611328: \n",
            "Epoch 3/10 Iteration 38/49 Loss 0.7650295495986938: \n",
            "Epoch 3/10 Iteration 39/49 Loss 0.7618918418884277: \n",
            "Epoch 3/10 Iteration 40/49 Loss 0.7218319177627563: \n",
            "Epoch 3/10 Iteration 41/49 Loss 0.7082711458206177: \n",
            "Epoch 3/10 Iteration 42/49 Loss 0.7391214966773987: \n",
            "Epoch 3/10 Iteration 43/49 Loss 0.7349976301193237: \n",
            "Epoch 3/10 Iteration 44/49 Loss 0.7606861591339111: \n",
            "Epoch 3/10 Iteration 45/49 Loss 0.79170823097229: \n",
            "Epoch 3/10 Iteration 46/49 Loss 0.7928756475448608: \n",
            "Epoch 3/10 Iteration 47/49 Loss 0.7223086357116699: \n",
            "Epoch 3/10 Iteration 48/49 Loss 0.7546355128288269: \n",
            "Validation Epoch 3/10 Iteration 9/10 accuracy 0.7406: \n",
            "Validation Epoch 3/10 Iteration 9/10 recall_mean 0.7406118737835836: \n",
            "Validation Epoch 3/10 Iteration 9/10 precision_mean 0.7369074952316471: \n",
            "Validation Epoch 3/10 Iteration 9/10 f1_mean 0.7358373410729261: \n",
            "Validation Epoch 3/10 Iteration 9/10 roc_auc_mean 0.9633916997610898: \n",
            "\n",
            "\n",
            "Epoch 4/10 Iteration 0/49 Loss 0.7700251340866089: \n",
            "Epoch 4/10 Iteration 1/49 Loss 0.7627576589584351: \n",
            "Epoch 4/10 Iteration 2/49 Loss 0.7269411087036133: \n",
            "Epoch 4/10 Iteration 3/49 Loss 0.7643855810165405: \n",
            "Epoch 4/10 Iteration 4/49 Loss 0.7739043235778809: \n",
            "Epoch 4/10 Iteration 5/49 Loss 0.7500103712081909: \n",
            "Epoch 4/10 Iteration 6/49 Loss 0.7294996976852417: \n",
            "Epoch 4/10 Iteration 7/49 Loss 0.7481479644775391: \n",
            "Epoch 4/10 Iteration 8/49 Loss 0.7712901830673218: \n",
            "Epoch 4/10 Iteration 9/49 Loss 0.735563337802887: \n",
            "Epoch 4/10 Iteration 10/49 Loss 0.7500994205474854: \n",
            "Epoch 4/10 Iteration 11/49 Loss 0.718580961227417: \n",
            "Epoch 4/10 Iteration 12/49 Loss 0.7264189124107361: \n",
            "Epoch 4/10 Iteration 13/49 Loss 0.7471710443496704: \n",
            "Epoch 4/10 Iteration 14/49 Loss 0.7357457876205444: \n",
            "Epoch 4/10 Iteration 15/49 Loss 0.695716142654419: \n",
            "Epoch 4/10 Iteration 16/49 Loss 0.7639745473861694: \n",
            "Epoch 4/10 Iteration 17/49 Loss 0.7080569863319397: \n",
            "Epoch 4/10 Iteration 18/49 Loss 0.7486183643341064: \n",
            "Epoch 4/10 Iteration 19/49 Loss 0.7388944625854492: \n",
            "Epoch 4/10 Iteration 20/49 Loss 0.7460393905639648: \n",
            "Epoch 4/10 Iteration 21/49 Loss 0.7068635821342468: \n",
            "Epoch 4/10 Iteration 22/49 Loss 0.7416062355041504: \n",
            "Epoch 4/10 Iteration 23/49 Loss 0.726420521736145: \n",
            "Epoch 4/10 Iteration 24/49 Loss 0.7936229109764099: \n",
            "Epoch 4/10 Iteration 25/49 Loss 0.7408608794212341: \n",
            "Epoch 4/10 Iteration 26/49 Loss 0.7207543849945068: \n",
            "Epoch 4/10 Iteration 27/49 Loss 0.7490844130516052: \n",
            "Epoch 4/10 Iteration 28/49 Loss 0.6966637372970581: \n",
            "Epoch 4/10 Iteration 29/49 Loss 0.7833989858627319: \n",
            "Epoch 4/10 Iteration 30/49 Loss 0.7160959243774414: \n",
            "Epoch 4/10 Iteration 31/49 Loss 0.6997805833816528: \n",
            "Epoch 4/10 Iteration 32/49 Loss 0.7168319821357727: \n",
            "Epoch 4/10 Iteration 33/49 Loss 0.7595481872558594: \n",
            "Epoch 4/10 Iteration 34/49 Loss 0.7191462516784668: \n",
            "Epoch 4/10 Iteration 35/49 Loss 0.6902822256088257: \n",
            "Epoch 4/10 Iteration 36/49 Loss 0.6903363466262817: \n",
            "Epoch 4/10 Iteration 37/49 Loss 0.7293259501457214: \n",
            "Epoch 4/10 Iteration 38/49 Loss 0.7204906344413757: \n",
            "Epoch 4/10 Iteration 39/49 Loss 0.7159478068351746: \n",
            "Epoch 4/10 Iteration 40/49 Loss 0.6775448322296143: \n",
            "Epoch 4/10 Iteration 41/49 Loss 0.6673306822776794: \n",
            "Epoch 4/10 Iteration 42/49 Loss 0.6948552131652832: \n",
            "Epoch 4/10 Iteration 43/49 Loss 0.6916670799255371: \n",
            "Epoch 4/10 Iteration 44/49 Loss 0.7175748348236084: \n",
            "Epoch 4/10 Iteration 45/49 Loss 0.7501528263092041: \n",
            "Epoch 4/10 Iteration 46/49 Loss 0.7506450414657593: \n",
            "Epoch 4/10 Iteration 47/49 Loss 0.6799252033233643: \n",
            "Epoch 4/10 Iteration 48/49 Loss 0.7133810520172119: \n",
            "Validation Epoch 4/10 Iteration 9/10 accuracy 0.7563: \n",
            "Validation Epoch 4/10 Iteration 9/10 recall_mean 0.7563855795769616: \n",
            "Validation Epoch 4/10 Iteration 9/10 precision_mean 0.7529256715726655: \n",
            "Validation Epoch 4/10 Iteration 9/10 f1_mean 0.7522761473996735: \n",
            "Validation Epoch 4/10 Iteration 9/10 roc_auc_mean 0.9664743771610359: \n",
            "\n",
            "\n",
            "Epoch 5/10 Iteration 0/49 Loss 0.7293674349784851: \n",
            "Epoch 5/10 Iteration 1/49 Loss 0.7236747741699219: \n",
            "Epoch 5/10 Iteration 2/49 Loss 0.687542200088501: \n",
            "Epoch 5/10 Iteration 3/49 Loss 0.7253418564796448: \n",
            "Epoch 5/10 Iteration 4/49 Loss 0.735161542892456: \n",
            "Epoch 5/10 Iteration 5/49 Loss 0.7122717499732971: \n",
            "Epoch 5/10 Iteration 6/49 Loss 0.6924481987953186: \n",
            "Epoch 5/10 Iteration 7/49 Loss 0.7104901671409607: \n",
            "Epoch 5/10 Iteration 8/49 Loss 0.7288488149642944: \n",
            "Epoch 5/10 Iteration 9/49 Loss 0.6944267153739929: \n",
            "Epoch 5/10 Iteration 10/49 Loss 0.7145197987556458: \n",
            "Epoch 5/10 Iteration 11/49 Loss 0.6817667484283447: \n",
            "Epoch 5/10 Iteration 12/49 Loss 0.6883273720741272: \n",
            "Epoch 5/10 Iteration 13/49 Loss 0.7090193033218384: \n",
            "Epoch 5/10 Iteration 14/49 Loss 0.6970915794372559: \n",
            "Epoch 5/10 Iteration 15/49 Loss 0.6602377891540527: \n",
            "Epoch 5/10 Iteration 16/49 Loss 0.7275436520576477: \n",
            "Epoch 5/10 Iteration 17/49 Loss 0.6719474792480469: \n",
            "Epoch 5/10 Iteration 18/49 Loss 0.7103536128997803: \n",
            "Epoch 5/10 Iteration 19/49 Loss 0.7027027606964111: \n",
            "Epoch 5/10 Iteration 20/49 Loss 0.710844874382019: \n",
            "Epoch 5/10 Iteration 21/49 Loss 0.6692134737968445: \n",
            "Epoch 5/10 Iteration 22/49 Loss 0.7078902721405029: \n",
            "Epoch 5/10 Iteration 23/49 Loss 0.6910867691040039: \n",
            "Epoch 5/10 Iteration 24/49 Loss 0.7603868246078491: \n",
            "Epoch 5/10 Iteration 25/49 Loss 0.7051048278808594: \n",
            "Epoch 5/10 Iteration 26/49 Loss 0.6865752935409546: \n",
            "Epoch 5/10 Iteration 27/49 Loss 0.7136256694793701: \n",
            "Epoch 5/10 Iteration 28/49 Loss 0.6618093252182007: \n",
            "Epoch 5/10 Iteration 29/49 Loss 0.7493730783462524: \n",
            "Epoch 5/10 Iteration 30/49 Loss 0.6828921437263489: \n",
            "Epoch 5/10 Iteration 31/49 Loss 0.6654877662658691: \n",
            "Epoch 5/10 Iteration 32/49 Loss 0.6852920055389404: \n",
            "Epoch 5/10 Iteration 33/49 Loss 0.7266213893890381: \n",
            "Epoch 5/10 Iteration 34/49 Loss 0.6856033205986023: \n",
            "Epoch 5/10 Iteration 35/49 Loss 0.6583982706069946: \n",
            "Epoch 5/10 Iteration 36/49 Loss 0.6581151485443115: \n",
            "Epoch 5/10 Iteration 37/49 Loss 0.6975759863853455: \n",
            "Epoch 5/10 Iteration 38/49 Loss 0.6873531341552734: \n",
            "Epoch 5/10 Iteration 39/49 Loss 0.682088315486908: \n",
            "Epoch 5/10 Iteration 40/49 Loss 0.6443494558334351: \n",
            "Epoch 5/10 Iteration 41/49 Loss 0.6370647549629211: \n",
            "Epoch 5/10 Iteration 42/49 Loss 0.6617415547370911: \n",
            "Epoch 5/10 Iteration 43/49 Loss 0.6591060161590576: \n",
            "Epoch 5/10 Iteration 44/49 Loss 0.6853189468383789: \n",
            "Epoch 5/10 Iteration 45/49 Loss 0.7188846468925476: \n",
            "Epoch 5/10 Iteration 46/49 Loss 0.7187656164169312: \n",
            "Epoch 5/10 Iteration 47/49 Loss 0.6482622623443604: \n",
            "Epoch 5/10 Iteration 48/49 Loss 0.681942343711853: \n",
            "Validation Epoch 5/10 Iteration 9/10 accuracy 0.7691: \n",
            "Validation Epoch 5/10 Iteration 9/10 recall_mean 0.7691099798001294: \n",
            "Validation Epoch 5/10 Iteration 9/10 precision_mean 0.7657005967843398: \n",
            "Validation Epoch 5/10 Iteration 9/10 f1_mean 0.7654729948550569: \n",
            "Validation Epoch 5/10 Iteration 9/10 roc_auc_mean 0.9687870424640052: \n",
            "\n",
            "\n",
            "Epoch 6/10 Iteration 0/49 Loss 0.6984384655952454: \n",
            "Epoch 6/10 Iteration 1/49 Loss 0.6944746375083923: \n",
            "Epoch 6/10 Iteration 2/49 Loss 0.6577751636505127: \n",
            "Epoch 6/10 Iteration 3/49 Loss 0.6960123777389526: \n",
            "Epoch 6/10 Iteration 4/49 Loss 0.7061015963554382: \n",
            "Epoch 6/10 Iteration 5/49 Loss 0.6836133003234863: \n",
            "Epoch 6/10 Iteration 6/49 Loss 0.6641751527786255: \n",
            "Epoch 6/10 Iteration 7/49 Loss 0.6818298101425171: \n",
            "Epoch 6/10 Iteration 8/49 Loss 0.6962994337081909: \n",
            "Epoch 6/10 Iteration 9/49 Loss 0.663021445274353: \n",
            "Epoch 6/10 Iteration 10/49 Loss 0.6872379779815674: \n",
            "Epoch 6/10 Iteration 11/49 Loss 0.6537976264953613: \n",
            "Epoch 6/10 Iteration 12/49 Loss 0.6591578125953674: \n",
            "Epoch 6/10 Iteration 13/49 Loss 0.6794373989105225: \n",
            "Epoch 6/10 Iteration 14/49 Loss 0.667793869972229: \n",
            "Epoch 6/10 Iteration 15/49 Loss 0.6334007978439331: \n",
            "Epoch 6/10 Iteration 16/49 Loss 0.6995231509208679: \n",
            "Epoch 6/10 Iteration 17/49 Loss 0.6443419456481934: \n",
            "Epoch 6/10 Iteration 18/49 Loss 0.68082594871521: \n",
            "Epoch 6/10 Iteration 19/49 Loss 0.6751381158828735: \n",
            "Epoch 6/10 Iteration 20/49 Loss 0.6837061643600464: \n",
            "Epoch 6/10 Iteration 21/49 Loss 0.6401234269142151: \n",
            "Epoch 6/10 Iteration 22/49 Loss 0.6818721294403076: \n",
            "Epoch 6/10 Iteration 23/49 Loss 0.6635651588439941: \n",
            "Epoch 6/10 Iteration 24/49 Loss 0.7347571849822998: \n",
            "Epoch 6/10 Iteration 25/49 Loss 0.6773338913917542: \n",
            "Epoch 6/10 Iteration 26/49 Loss 0.6601042151451111: \n",
            "Epoch 6/10 Iteration 27/49 Loss 0.6861563324928284: \n",
            "Epoch 6/10 Iteration 28/49 Loss 0.6348230838775635: \n",
            "Epoch 6/10 Iteration 29/49 Loss 0.7224645018577576: \n",
            "Epoch 6/10 Iteration 30/49 Loss 0.6573671102523804: \n",
            "Epoch 6/10 Iteration 31/49 Loss 0.6386865377426147: \n",
            "Epoch 6/10 Iteration 32/49 Loss 0.6610897779464722: \n",
            "Epoch 6/10 Iteration 33/49 Loss 0.7009507417678833: \n",
            "Epoch 6/10 Iteration 34/49 Loss 0.6594298481941223: \n",
            "Epoch 6/10 Iteration 35/49 Loss 0.6336004734039307: \n",
            "Epoch 6/10 Iteration 36/49 Loss 0.6328032612800598: \n",
            "Epoch 6/10 Iteration 37/49 Loss 0.6726489663124084: \n",
            "Epoch 6/10 Iteration 38/49 Loss 0.6612066030502319: \n",
            "Epoch 6/10 Iteration 39/49 Loss 0.6556769609451294: \n",
            "Epoch 6/10 Iteration 40/49 Loss 0.618243932723999: \n",
            "Epoch 6/10 Iteration 41/49 Loss 0.6136448383331299: \n",
            "Epoch 6/10 Iteration 42/49 Loss 0.6358281970024109: \n",
            "Epoch 6/10 Iteration 43/49 Loss 0.633354663848877: \n",
            "Epoch 6/10 Iteration 44/49 Loss 0.6600154638290405: \n",
            "Epoch 6/10 Iteration 45/49 Loss 0.6942897439002991: \n",
            "Epoch 6/10 Iteration 46/49 Loss 0.6934937238693237: \n",
            "Epoch 6/10 Iteration 47/49 Loss 0.6235460042953491: \n",
            "Epoch 6/10 Iteration 48/49 Loss 0.6569123268127441: \n",
            "Validation Epoch 6/10 Iteration 9/10 accuracy 0.7776: \n",
            "Validation Epoch 6/10 Iteration 9/10 recall_mean 0.7776455620408287: \n",
            "Validation Epoch 6/10 Iteration 9/10 precision_mean 0.774118464459889: \n",
            "Validation Epoch 6/10 Iteration 9/10 f1_mean 0.7741726736898804: \n",
            "Validation Epoch 6/10 Iteration 9/10 roc_auc_mean 0.9705431091892122: \n",
            "\n",
            "\n",
            "Epoch 7/10 Iteration 0/49 Loss 0.6737995147705078: \n",
            "Epoch 7/10 Iteration 1/49 Loss 0.6716603636741638: \n",
            "Epoch 7/10 Iteration 2/49 Loss 0.6342656016349792: \n",
            "Epoch 7/10 Iteration 3/49 Loss 0.6727761030197144: \n",
            "Epoch 7/10 Iteration 4/49 Loss 0.6831581592559814: \n",
            "Epoch 7/10 Iteration 5/49 Loss 0.6610063314437866: \n",
            "Epoch 7/10 Iteration 6/49 Loss 0.6417043209075928: \n",
            "Epoch 7/10 Iteration 7/49 Loss 0.659024715423584: \n",
            "Epoch 7/10 Iteration 8/49 Loss 0.6702184081077576: \n",
            "Epoch 7/10 Iteration 9/49 Loss 0.6380082368850708: \n",
            "Epoch 7/10 Iteration 10/49 Loss 0.6654390692710876: \n",
            "Epoch 7/10 Iteration 11/49 Loss 0.631621778011322: \n",
            "Epoch 7/10 Iteration 12/49 Loss 0.6358351707458496: \n",
            "Epoch 7/10 Iteration 13/49 Loss 0.655725359916687: \n",
            "Epoch 7/10 Iteration 14/49 Loss 0.6443701982498169: \n",
            "Epoch 7/10 Iteration 15/49 Loss 0.6123926043510437: \n",
            "Epoch 7/10 Iteration 16/49 Loss 0.6770331859588623: \n",
            "Epoch 7/10 Iteration 17/49 Loss 0.622475802898407: \n",
            "Epoch 7/10 Iteration 18/49 Loss 0.6571764945983887: \n",
            "Epoch 7/10 Iteration 19/49 Loss 0.653342604637146: \n",
            "Epoch 7/10 Iteration 20/49 Loss 0.6619549989700317: \n",
            "Epoch 7/10 Iteration 21/49 Loss 0.6166850328445435: \n",
            "Epoch 7/10 Iteration 22/49 Loss 0.6610767841339111: \n",
            "Epoch 7/10 Iteration 23/49 Loss 0.6413261294364929: \n",
            "Epoch 7/10 Iteration 24/49 Loss 0.7142353057861328: \n",
            "Epoch 7/10 Iteration 25/49 Loss 0.6548797488212585: \n",
            "Epoch 7/10 Iteration 26/49 Loss 0.6389908194541931: \n",
            "Epoch 7/10 Iteration 27/49 Loss 0.6640146970748901: \n",
            "Epoch 7/10 Iteration 28/49 Loss 0.61322021484375: \n",
            "Epoch 7/10 Iteration 29/49 Loss 0.7005835771560669: \n",
            "Epoch 7/10 Iteration 30/49 Loss 0.6368352174758911: \n",
            "Epoch 7/10 Iteration 31/49 Loss 0.6171314716339111: \n",
            "Epoch 7/10 Iteration 32/49 Loss 0.6418440341949463: \n",
            "Epoch 7/10 Iteration 33/49 Loss 0.6802300214767456: \n",
            "Epoch 7/10 Iteration 34/49 Loss 0.6382607817649841: \n",
            "Epoch 7/10 Iteration 35/49 Loss 0.6135517954826355: \n",
            "Epoch 7/10 Iteration 36/49 Loss 0.6122504472732544: \n",
            "Epoch 7/10 Iteration 37/49 Loss 0.6524306535720825: \n",
            "Epoch 7/10 Iteration 38/49 Loss 0.6400444507598877: \n",
            "Epoch 7/10 Iteration 39/49 Loss 0.6343953013420105: \n",
            "Epoch 7/10 Iteration 40/49 Loss 0.5971171855926514: \n",
            "Epoch 7/10 Iteration 41/49 Loss 0.594886064529419: \n",
            "Epoch 7/10 Iteration 42/49 Loss 0.6148219704627991: \n",
            "Epoch 7/10 Iteration 43/49 Loss 0.6122994422912598: \n",
            "Epoch 7/10 Iteration 44/49 Loss 0.6395370960235596: \n",
            "Epoch 7/10 Iteration 45/49 Loss 0.6742380857467651: \n",
            "Epoch 7/10 Iteration 46/49 Loss 0.6727911829948425: \n",
            "Epoch 7/10 Iteration 47/49 Loss 0.6035961508750916: \n",
            "Epoch 7/10 Iteration 48/49 Loss 0.6363238096237183: \n",
            "Validation Epoch 7/10 Iteration 9/10 accuracy 0.7846: \n",
            "Validation Epoch 7/10 Iteration 9/10 recall_mean 0.7846588381735902: \n",
            "Validation Epoch 7/10 Iteration 9/10 precision_mean 0.7813482049582129: \n",
            "Validation Epoch 7/10 Iteration 9/10 f1_mean 0.7814974883060977: \n",
            "Validation Epoch 7/10 Iteration 9/10 roc_auc_mean 0.9719260880834005: \n",
            "\n",
            "\n",
            "Epoch 8/10 Iteration 0/49 Loss 0.6536478996276855: \n",
            "Epoch 8/10 Iteration 1/49 Loss 0.6532255411148071: \n",
            "Epoch 8/10 Iteration 2/49 Loss 0.6152046918869019: \n",
            "Epoch 8/10 Iteration 3/49 Loss 0.6539419889450073: \n",
            "Epoch 8/10 Iteration 4/49 Loss 0.6645971536636353: \n",
            "Epoch 8/10 Iteration 5/49 Loss 0.6426739692687988: \n",
            "Epoch 8/10 Iteration 6/49 Loss 0.6232149600982666: \n",
            "Epoch 8/10 Iteration 7/49 Loss 0.6402164697647095: \n",
            "Epoch 8/10 Iteration 8/49 Loss 0.6485673189163208: \n",
            "Epoch 8/10 Iteration 9/49 Loss 0.6176322102546692: \n",
            "Epoch 8/10 Iteration 10/49 Loss 0.6475808620452881: \n",
            "Epoch 8/10 Iteration 11/49 Loss 0.61344313621521: \n",
            "Epoch 8/10 Iteration 12/49 Loss 0.616596519947052: \n",
            "Epoch 8/10 Iteration 13/49 Loss 0.636057436466217: \n",
            "Epoch 8/10 Iteration 14/49 Loss 0.6251932382583618: \n",
            "Epoch 8/10 Iteration 15/49 Loss 0.595378041267395: \n",
            "Epoch 8/10 Iteration 16/49 Loss 0.6585413217544556: \n",
            "Epoch 8/10 Iteration 17/49 Loss 0.604574978351593: \n",
            "Epoch 8/10 Iteration 18/49 Loss 0.6375917196273804: \n",
            "Epoch 8/10 Iteration 19/49 Loss 0.635568380355835: \n",
            "Epoch 8/10 Iteration 20/49 Loss 0.6441475749015808: \n",
            "Epoch 8/10 Iteration 21/49 Loss 0.5972920656204224: \n",
            "Epoch 8/10 Iteration 22/49 Loss 0.6439478397369385: \n",
            "Epoch 8/10 Iteration 23/49 Loss 0.6227912902832031: \n",
            "Epoch 8/10 Iteration 24/49 Loss 0.6973186731338501: \n",
            "Epoch 8/10 Iteration 25/49 Loss 0.6362622976303101: \n",
            "Epoch 8/10 Iteration 26/49 Loss 0.6215763092041016: \n",
            "Epoch 8/10 Iteration 27/49 Loss 0.6457096338272095: \n",
            "Epoch 8/10 Iteration 28/49 Loss 0.5952078104019165: \n",
            "Epoch 8/10 Iteration 29/49 Loss 0.6822269558906555: \n",
            "Epoch 8/10 Iteration 30/49 Loss 0.6198062300682068: \n",
            "Epoch 8/10 Iteration 31/49 Loss 0.5993174910545349: \n",
            "Epoch 8/10 Iteration 32/49 Loss 0.6260510683059692: \n",
            "Epoch 8/10 Iteration 33/49 Loss 0.6628460884094238: \n",
            "Epoch 8/10 Iteration 34/49 Loss 0.6205951571464539: \n",
            "Epoch 8/10 Iteration 35/49 Loss 0.5969271063804626: \n",
            "Epoch 8/10 Iteration 36/49 Loss 0.5951383113861084: \n",
            "Epoch 8/10 Iteration 37/49 Loss 0.6354727149009705: \n",
            "Epoch 8/10 Iteration 38/49 Loss 0.6223829984664917: \n",
            "Epoch 8/10 Iteration 39/49 Loss 0.6167888045310974: \n",
            "Epoch 8/10 Iteration 40/49 Loss 0.5793774724006653: \n",
            "Epoch 8/10 Iteration 41/49 Loss 0.5793982744216919: \n",
            "Epoch 8/10 Iteration 42/49 Loss 0.5973172783851624: \n",
            "Epoch 8/10 Iteration 43/49 Loss 0.5946987867355347: \n",
            "Epoch 8/10 Iteration 44/49 Loss 0.6224925518035889: \n",
            "Epoch 8/10 Iteration 45/49 Loss 0.6573421955108643: \n",
            "Epoch 8/10 Iteration 46/49 Loss 0.6554685235023499: \n",
            "Epoch 8/10 Iteration 47/49 Loss 0.5869883298873901: \n",
            "Epoch 8/10 Iteration 48/49 Loss 0.6188295483589172: \n",
            "Validation Epoch 8/10 Iteration 9/10 accuracy 0.7903: \n",
            "Validation Epoch 8/10 Iteration 9/10 recall_mean 0.7904377318268643: \n",
            "Validation Epoch 8/10 Iteration 9/10 precision_mean 0.7873331404046795: \n",
            "Validation Epoch 8/10 Iteration 9/10 f1_mean 0.7874750182388782: \n",
            "Validation Epoch 8/10 Iteration 9/10 roc_auc_mean 0.9730660865705831: \n",
            "\n",
            "\n",
            "Epoch 9/10 Iteration 0/49 Loss 0.6367921829223633: \n",
            "Epoch 9/10 Iteration 1/49 Loss 0.6377803087234497: \n",
            "Epoch 9/10 Iteration 2/49 Loss 0.5994071960449219: \n",
            "Epoch 9/10 Iteration 3/49 Loss 0.6382164359092712: \n",
            "Epoch 9/10 Iteration 4/49 Loss 0.6491648554801941: \n",
            "Epoch 9/10 Iteration 5/49 Loss 0.6273082494735718: \n",
            "Epoch 9/10 Iteration 6/49 Loss 0.6076956987380981: \n",
            "Epoch 9/10 Iteration 7/49 Loss 0.6242833733558655: \n",
            "Epoch 9/10 Iteration 8/49 Loss 0.6302528381347656: \n",
            "Epoch 9/10 Iteration 9/49 Loss 0.6005263328552246: \n",
            "Epoch 9/10 Iteration 10/49 Loss 0.6326329112052917: \n",
            "Epoch 9/10 Iteration 11/49 Loss 0.5981957912445068: \n",
            "Epoch 9/10 Iteration 12/49 Loss 0.6002963781356812: \n",
            "Epoch 9/10 Iteration 13/49 Loss 0.6192894577980042: \n",
            "Epoch 9/10 Iteration 14/49 Loss 0.6090542674064636: \n",
            "Epoch 9/10 Iteration 15/49 Loss 0.5812028646469116: \n",
            "Epoch 9/10 Iteration 16/49 Loss 0.6430000066757202: \n",
            "Epoch 9/10 Iteration 17/49 Loss 0.5894525051116943: \n",
            "Epoch 9/10 Iteration 18/49 Loss 0.6209673881530762: \n",
            "Epoch 9/10 Iteration 19/49 Loss 0.620619535446167: \n",
            "Epoch 9/10 Iteration 20/49 Loss 0.6292206048965454: \n",
            "Epoch 9/10 Iteration 21/49 Loss 0.5809401869773865: \n",
            "Epoch 9/10 Iteration 22/49 Loss 0.6294883489608765: \n",
            "Epoch 9/10 Iteration 23/49 Loss 0.6070551872253418: \n",
            "Epoch 9/10 Iteration 24/49 Loss 0.6830211877822876: \n",
            "Epoch 9/10 Iteration 25/49 Loss 0.6204231381416321: \n",
            "Epoch 9/10 Iteration 26/49 Loss 0.6069451570510864: \n",
            "Epoch 9/10 Iteration 27/49 Loss 0.6302125453948975: \n",
            "Epoch 9/10 Iteration 28/49 Loss 0.5799518823623657: \n",
            "Epoch 9/10 Iteration 29/49 Loss 0.6665282845497131: \n",
            "Epoch 9/10 Iteration 30/49 Loss 0.6053578853607178: \n",
            "Epoch 9/10 Iteration 31/49 Loss 0.584257960319519: \n",
            "Epoch 9/10 Iteration 32/49 Loss 0.6128131747245789: \n",
            "Epoch 9/10 Iteration 33/49 Loss 0.6480516791343689: \n",
            "Epoch 9/10 Iteration 34/49 Loss 0.6055376529693604: \n",
            "Epoch 9/10 Iteration 35/49 Loss 0.582840085029602: \n",
            "Epoch 9/10 Iteration 36/49 Loss 0.5805472731590271: \n",
            "Epoch 9/10 Iteration 37/49 Loss 0.6210415363311768: \n",
            "Epoch 9/10 Iteration 38/49 Loss 0.6073556542396545: \n",
            "Epoch 9/10 Iteration 39/49 Loss 0.6019057631492615: \n",
            "Epoch 9/10 Iteration 40/49 Loss 0.5642386674880981: \n",
            "Epoch 9/10 Iteration 41/49 Loss 0.5663822889328003: \n",
            "Epoch 9/10 Iteration 42/49 Loss 0.5824779868125916: \n",
            "Epoch 9/10 Iteration 43/49 Loss 0.579616129398346: \n",
            "Epoch 9/10 Iteration 44/49 Loss 0.6080251932144165: \n",
            "Epoch 9/10 Iteration 45/49 Loss 0.642855703830719: \n",
            "Epoch 9/10 Iteration 46/49 Loss 0.6406800150871277: \n",
            "Epoch 9/10 Iteration 47/49 Loss 0.5728816390037537: \n",
            "Epoch 9/10 Iteration 48/49 Loss 0.6038002371788025: \n",
            "Validation Epoch 9/10 Iteration 9/10 accuracy 0.7944: \n",
            "Validation Epoch 9/10 Iteration 9/10 recall_mean 0.794344022140685: \n",
            "Validation Epoch 9/10 Iteration 9/10 precision_mean 0.7914939458596633: \n",
            "Validation Epoch 9/10 Iteration 9/10 f1_mean 0.7915690640958781: \n",
            "Validation Epoch 9/10 Iteration 9/10 roc_auc_mean 0.9740256405126896: \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Neural Network\n",
        "  + Change the *seed* and train the neural network 10 times\n",
        "  + Report the mean and variance over 10 trials on the following metrics:\n",
        "    + Accuracy\n",
        "    + Precision\n",
        "    + Recall\n",
        "    + F1\n",
        "    + ROC curve\n",
        "\n"
      ],
      "metadata": {
        "id": "JXV_Q44g9ese"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write Relevant training and reporting code here:\n",
        "#----------------------------------------------------\n",
        "import random\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "rocs = []\n",
        "\n",
        "for i in range(10):\n",
        "  seed = random.randrange(150, 700)\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  model = NeuralNetwork().cuda()\n",
        "\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    for idx, data in enumerate(train_loader):\n",
        "      features, labels = data\n",
        "\n",
        "      features = features.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "      probs = model(features.reshape([-1, 784]))\n",
        "\n",
        "      loss = ce_loss(probs, labels)\n",
        "      training_loss.append(loss.item())\n",
        "\n",
        "      for param in model.parameters():\n",
        "        param.grad = None\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      for name, param in model.named_parameters():\n",
        "        new_param = param - learning_rate * param.grad\n",
        "\n",
        "        with torch.no_grad():\n",
        "          param.copy_(new_param)\n",
        "\n",
        "  accuracy_total = 0\n",
        "  total = 0\n",
        "\n",
        "  recall_total = []\n",
        "  precision_total = []\n",
        "  f1_total = []\n",
        "  roc_auc_total = []\n",
        "\n",
        "  for idx, data in enumerate(test_loader):\n",
        "    features, labels = data\n",
        "\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      probs = model(features.reshape([-1, 784]))\n",
        "\n",
        "      loss = ce_loss(probs, labels)\n",
        "      validation_loss.append(loss.item())\n",
        "\n",
        "      max_prob = torch.argmax(probs, dim=1)\n",
        "\n",
        "      accuracy_total += sum(max_prob == labels).item()\n",
        "      total += labels.shape[0]\n",
        "      y_true = labels.cpu().numpy()\n",
        "      y_pred = probs.argmax(axis=1).cpu().numpy()\n",
        "      y_score = probs.cpu().detach().numpy()\n",
        "\n",
        "      precision = precision_score(y_true, y_pred,average ='macro')\n",
        "      recall = recall_score(y_true, y_pred, average ='macro')\n",
        "      f1 = f1_score(y_true, y_pred,average = 'macro')\n",
        "      roc_auc = roc_auc_score(y_true, y_score,multi_class='ovr')\n",
        "\n",
        "    recall_total.append(recall)\n",
        "    precision_total.append(precision)\n",
        "    f1_total.append(f1)\n",
        "    roc_auc_total.append(roc_auc)\n",
        "\n",
        "  accuracy = accuracy_total/total\n",
        "\n",
        "  accuracies.append(accuracy)\n",
        "  precisions.append(sum(precision_total)/len(precision_total))\n",
        "  recalls.append(sum(recall_total)/len(recall_total))\n",
        "  f1s.append(sum(f1_total)/len(f1_total))\n",
        "  rocs.append(sum(roc_auc_total)/len(roc_auc_total))\n",
        "\n",
        "  print(f\"seed = {seed} | accuracy={accuracy}, precision={precision}, recall={recall}, f1={f1}, roc_auc={roc_auc}\")\n",
        "  print(\"\")\n",
        "\n",
        "print(f\"\\nAccuracy-> Mean: {sum(accuracies)/len(accuracies)}, Variance: {np.var(accuracies)}\")\n",
        "print(f\"Precision-> Mean: {sum(precisions)/len(precisions)}, Variance: {np.var(precisions)}\")\n",
        "print(f\"Recall-> Mean: {sum(recalls)/len(recalls)}, Variance: {np.var(recalls)}\")\n",
        "print(f\"F1-> Mean: {sum(f1s)/len(f1s)}, Variance: {np.var(f1s)}\")\n",
        "print(f\"ROC-> Mean: {sum(rocs)/len(rocs)}, Variance: {np.var(rocs)}\")"
      ],
      "metadata": {
        "id": "wCBNOBOK9o1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c581cd69-dd14-4961-f112-fad8ddcf4c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed = 601 | accuracy=0.7838, precision=0.7907998230898294, recall=0.7919437355826745, f1=0.7878292667090396, roc_auc=0.9723935777139723\n",
            "\n",
            "seed = 347 | accuracy=0.7829, precision=0.7796870568679034, recall=0.7827882328525836, f1=0.7781365542692459, roc_auc=0.9719816133170311\n",
            "\n",
            "seed = 508 | accuracy=0.7901, precision=0.7911002025172534, recall=0.7940156506058414, f1=0.7911637901244217, roc_auc=0.9737642894103086\n",
            "\n",
            "seed = 579 | accuracy=0.7865, precision=0.790436921567204, recall=0.7905809839261172, f1=0.7889180656859652, roc_auc=0.9739233278893451\n",
            "\n",
            "seed = 473 | accuracy=0.7857, precision=0.783939693647038, recall=0.7869658583772564, f1=0.7842724798421433, roc_auc=0.9713335230215258\n",
            "\n",
            "seed = 494 | accuracy=0.7824, precision=0.7792968895136851, recall=0.7859250450804995, f1=0.7807007517016054, roc_auc=0.9711918149663175\n",
            "\n",
            "seed = 303 | accuracy=0.7803, precision=0.7837314788673485, recall=0.7829731277435733, f1=0.7824070309172954, roc_auc=0.9718236685479497\n",
            "\n",
            "seed = 182 | accuracy=0.7832, precision=0.7713968982245134, recall=0.775421558134451, f1=0.7712871513264249, roc_auc=0.9700648188061258\n",
            "\n",
            "seed = 612 | accuracy=0.7838, precision=0.7953041276007908, recall=0.7992275808739411, f1=0.7962224949493174, roc_auc=0.9729581237045638\n",
            "\n",
            "seed = 380 | accuracy=0.7862, precision=0.7773122568740937, recall=0.7789470648835118, f1=0.7768386568027202, roc_auc=0.9715654981410344\n",
            "\n",
            "\n",
            "Accuracy-> Mean: 0.78449, Variance: 6.6968999999999934e-06\n",
            "Precision-> Mean: 0.7826139877966626, Variance: 6.461277027906235e-06\n",
            "Recall-> Mean: 0.7845466982140188, Variance: 6.793497098136442e-06\n",
            "F1-> Mean: 0.7820320272862467, Variance: 7.011605073521725e-06\n",
            "ROC-> Mean: 0.9718126327276222, Variance: 5.846663758706842e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Change the number of parameters in the Neural Network\n",
        "  + Train an underfit model\n",
        "  + Train a overfit model\n",
        "  + Demostrate the difference between overfitting and underfitting. Give detailed reasoning."
      ],
      "metadata": {
        "id": "0W6ZCfma9iTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create new models here with different number of parameters, discuss and demonstrate underfitting and overfitting in NN\n",
        "#----------------------------------------------------\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "class OverFittingModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(OverFittingModel, self).__init__()\n",
        "    # Adding 4 layers for overfitting\n",
        "    self.layer_1 = CustomLinearLayer(784, 512)\n",
        "    self.layer_2 = CustomLinearLayer(512, 400)\n",
        "    self.layer_3 = CustomLinearLayer(400, 150)\n",
        "    self.layer_4 = CustomLinearLayer(150, 10)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output_1 = self.layer_1(x)\n",
        "    output_1_actv = self.activation(output_1)\n",
        "    output_2 = self.layer_2(output_1_actv)\n",
        "    output_2_actv = self.activation(output_2)\n",
        "    output_3 = self.layer_3(output_2_actv)\n",
        "    output_3_actv = self.activation(output_3)\n",
        "    output_4 = self.layer_4(output_3_actv)\n",
        "    output = self.softmax(output_4)\n",
        "    return output\n",
        "\n",
        "modelOverFitting = OverFittingModel().cuda()\n",
        "ce_loss = CrossEntropyLoss()\n",
        "\n",
        "train_iter_loss =[]\n",
        "test_iter_loss = []\n",
        "training_loss = []\n",
        "testing_loss = []\n",
        "max_epochs = 21\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  for idx, data in enumerate(train_loader):\n",
        "    features, labels = data\n",
        "    features = features.cuda()\n",
        "\n",
        "    labels = labels.cuda()\n",
        "    probs = modelOverFitting(features.reshape([-1, 784]))\n",
        "\n",
        "    loss = ce_loss(probs, labels)\n",
        "    train_iter_loss.append(loss.item())\n",
        "    print(\"Epoch {0}/{1} Iteration {2}/{3} Loss {4}: \".format(epoch, max_epochs, idx, len(train_loader), loss))\n",
        "\n",
        "    for param in modelOverFitting.parameters():\n",
        "      param.grad = None\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in modelOverFitting.named_parameters():\n",
        "      # Write paramtere update routine here:\n",
        "      # --------------\n",
        "      new_param = param - learning_rate * param.grad\n",
        "\n",
        "      # --------------\n",
        "      with torch.no_grad():\n",
        "        param.copy_(new_param)\n",
        "  training_loss.append(torch.mean(torch.Tensor(train_iter_loss)))\n",
        "\n",
        "  total = 0\n",
        "  accuracy_total = 0\n",
        "  recall_total = []\n",
        "  precision_total = []\n",
        "  f1_total = []\n",
        "  roc_auc_total = []\n",
        "\n",
        "  # Write Testing routine here:\n",
        "  #----------------------------\n",
        "  for idx, data in enumerate(test_loader):\n",
        "    features, labels = data\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      probs = modelOverFitting(features.reshape([-1, 784]))\n",
        "      loss = ce_loss(probs, labels)\n",
        "      test_iter_loss.append(loss.item())\n",
        "\n",
        "  testing_loss.append(torch.mean(torch.Tensor(test_iter_loss)))\n",
        "plt.plot(training_loss, label='Training Loss')\n",
        "plt.plot(testing_loss, label='Testing Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a1ae0b2-907d-4386-f724-ad068b0b5bf3",
        "id": "S90JXV8tKfEv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/21 Iteration 0/49 Loss 5.198268890380859: \n",
            "Epoch 0/21 Iteration 1/49 Loss 3.698981285095215: \n",
            "Epoch 0/21 Iteration 2/49 Loss 2.8212122917175293: \n",
            "Epoch 0/21 Iteration 3/49 Loss 2.241880178451538: \n",
            "Epoch 0/21 Iteration 4/49 Loss 2.03098201751709: \n",
            "Epoch 0/21 Iteration 5/49 Loss 1.9312150478363037: \n",
            "Epoch 0/21 Iteration 6/49 Loss 1.8193764686584473: \n",
            "Epoch 0/21 Iteration 7/49 Loss 1.7383151054382324: \n",
            "Epoch 0/21 Iteration 8/49 Loss 1.6587705612182617: \n",
            "Epoch 0/21 Iteration 9/49 Loss 1.6116873025894165: \n",
            "Epoch 0/21 Iteration 10/49 Loss 1.5005861520767212: \n",
            "Epoch 0/21 Iteration 11/49 Loss 1.4743459224700928: \n",
            "Epoch 0/21 Iteration 12/49 Loss 1.4294978380203247: \n",
            "Epoch 0/21 Iteration 13/49 Loss 1.4062213897705078: \n",
            "Epoch 0/21 Iteration 14/49 Loss 1.3547742366790771: \n",
            "Epoch 0/21 Iteration 15/49 Loss 1.2816632986068726: \n",
            "Epoch 0/21 Iteration 16/49 Loss 1.3524682521820068: \n",
            "Epoch 0/21 Iteration 17/49 Loss 1.2516263723373413: \n",
            "Epoch 0/21 Iteration 18/49 Loss 1.2652673721313477: \n",
            "Epoch 0/21 Iteration 19/49 Loss 1.2387815713882446: \n",
            "Epoch 0/21 Iteration 20/49 Loss 1.20098876953125: \n",
            "Epoch 0/21 Iteration 21/49 Loss 1.2089297771453857: \n",
            "Epoch 0/21 Iteration 22/49 Loss 1.1777920722961426: \n",
            "Epoch 0/21 Iteration 23/49 Loss 1.1419786214828491: \n",
            "Epoch 0/21 Iteration 24/49 Loss 1.1932408809661865: \n",
            "Epoch 0/21 Iteration 25/49 Loss 1.1186962127685547: \n",
            "Epoch 0/21 Iteration 26/49 Loss 1.1115880012512207: \n",
            "Epoch 0/21 Iteration 27/49 Loss 1.0899661779403687: \n",
            "Epoch 0/21 Iteration 28/49 Loss 1.0342602729797363: \n",
            "Epoch 0/21 Iteration 29/49 Loss 1.104994773864746: \n",
            "Epoch 0/21 Iteration 30/49 Loss 1.024293303489685: \n",
            "Epoch 0/21 Iteration 31/49 Loss 1.016768217086792: \n",
            "Epoch 0/21 Iteration 32/49 Loss 1.0242719650268555: \n",
            "Epoch 0/21 Iteration 33/49 Loss 1.0611531734466553: \n",
            "Epoch 0/21 Iteration 34/49 Loss 1.0048794746398926: \n",
            "Epoch 0/21 Iteration 35/49 Loss 0.9761604070663452: \n",
            "Epoch 0/21 Iteration 36/49 Loss 0.958463728427887: \n",
            "Epoch 0/21 Iteration 37/49 Loss 0.9822413921356201: \n",
            "Epoch 0/21 Iteration 38/49 Loss 0.9629065990447998: \n",
            "Epoch 0/21 Iteration 39/49 Loss 0.9475226402282715: \n",
            "Epoch 0/21 Iteration 40/49 Loss 0.9262101650238037: \n",
            "Epoch 0/21 Iteration 41/49 Loss 0.9293501973152161: \n",
            "Epoch 0/21 Iteration 42/49 Loss 0.944803774356842: \n",
            "Epoch 0/21 Iteration 43/49 Loss 0.9190945625305176: \n",
            "Epoch 0/21 Iteration 44/49 Loss 0.9427919387817383: \n",
            "Epoch 0/21 Iteration 45/49 Loss 0.9643427133560181: \n",
            "Epoch 0/21 Iteration 46/49 Loss 0.9552358388900757: \n",
            "Epoch 0/21 Iteration 47/49 Loss 0.9119555950164795: \n",
            "Epoch 0/21 Iteration 48/49 Loss 0.9300931692123413: \n",
            "Epoch 1/21 Iteration 0/49 Loss 0.8833529949188232: \n",
            "Epoch 1/21 Iteration 1/49 Loss 0.9052542448043823: \n",
            "Epoch 1/21 Iteration 2/49 Loss 0.8803642988204956: \n",
            "Epoch 1/21 Iteration 3/49 Loss 0.8816797137260437: \n",
            "Epoch 1/21 Iteration 4/49 Loss 0.9001535773277283: \n",
            "Epoch 1/21 Iteration 5/49 Loss 0.9251550436019897: \n",
            "Epoch 1/21 Iteration 6/49 Loss 0.82233065366745: \n",
            "Epoch 1/21 Iteration 7/49 Loss 0.8599976301193237: \n",
            "Epoch 1/21 Iteration 8/49 Loss 0.8604903817176819: \n",
            "Epoch 1/21 Iteration 9/49 Loss 0.8862828016281128: \n",
            "Epoch 1/21 Iteration 10/49 Loss 0.8383663296699524: \n",
            "Epoch 1/21 Iteration 11/49 Loss 0.8201775550842285: \n",
            "Epoch 1/21 Iteration 12/49 Loss 0.847752571105957: \n",
            "Epoch 1/21 Iteration 13/49 Loss 0.8510788679122925: \n",
            "Epoch 1/21 Iteration 14/49 Loss 0.828704833984375: \n",
            "Epoch 1/21 Iteration 15/49 Loss 0.7875970005989075: \n",
            "Epoch 1/21 Iteration 16/49 Loss 0.8634111881256104: \n",
            "Epoch 1/21 Iteration 17/49 Loss 0.7918109893798828: \n",
            "Epoch 1/21 Iteration 18/49 Loss 0.8326327800750732: \n",
            "Epoch 1/21 Iteration 19/49 Loss 0.8376237154006958: \n",
            "Epoch 1/21 Iteration 20/49 Loss 0.8230662941932678: \n",
            "Epoch 1/21 Iteration 21/49 Loss 0.8028138875961304: \n",
            "Epoch 1/21 Iteration 22/49 Loss 0.821043074131012: \n",
            "Epoch 1/21 Iteration 23/49 Loss 0.7924465537071228: \n",
            "Epoch 1/21 Iteration 24/49 Loss 0.8606133460998535: \n",
            "Epoch 1/21 Iteration 25/49 Loss 0.8041003346443176: \n",
            "Epoch 1/21 Iteration 26/49 Loss 0.7913547158241272: \n",
            "Epoch 1/21 Iteration 27/49 Loss 0.7950054407119751: \n",
            "Epoch 1/21 Iteration 28/49 Loss 0.7395813465118408: \n",
            "Epoch 1/21 Iteration 29/49 Loss 0.8277472257614136: \n",
            "Epoch 1/21 Iteration 30/49 Loss 0.7567773461341858: \n",
            "Epoch 1/21 Iteration 31/49 Loss 0.7458390593528748: \n",
            "Epoch 1/21 Iteration 32/49 Loss 0.7766773104667664: \n",
            "Epoch 1/21 Iteration 33/49 Loss 0.8181190490722656: \n",
            "Epoch 1/21 Iteration 34/49 Loss 0.7635751366615295: \n",
            "Epoch 1/21 Iteration 35/49 Loss 0.741086483001709: \n",
            "Epoch 1/21 Iteration 36/49 Loss 0.7277101278305054: \n",
            "Epoch 1/21 Iteration 37/49 Loss 0.7679616212844849: \n",
            "Epoch 1/21 Iteration 38/49 Loss 0.7416802048683167: \n",
            "Epoch 1/21 Iteration 39/49 Loss 0.7343178987503052: \n",
            "Epoch 1/21 Iteration 40/49 Loss 0.7121237516403198: \n",
            "Epoch 1/21 Iteration 41/49 Loss 0.7317588925361633: \n",
            "Epoch 1/21 Iteration 42/49 Loss 0.7489254474639893: \n",
            "Epoch 1/21 Iteration 43/49 Loss 0.7235726118087769: \n",
            "Epoch 1/21 Iteration 44/49 Loss 0.7531490325927734: \n",
            "Epoch 1/21 Iteration 45/49 Loss 0.7770451307296753: \n",
            "Epoch 1/21 Iteration 46/49 Loss 0.7850998044013977: \n",
            "Epoch 1/21 Iteration 47/49 Loss 0.7307690382003784: \n",
            "Epoch 1/21 Iteration 48/49 Loss 0.7561727166175842: \n",
            "Epoch 2/21 Iteration 0/49 Loss 0.7306432723999023: \n",
            "Epoch 2/21 Iteration 1/49 Loss 0.7503620386123657: \n",
            "Epoch 2/21 Iteration 2/49 Loss 0.7186359763145447: \n",
            "Epoch 2/21 Iteration 3/49 Loss 0.7286516427993774: \n",
            "Epoch 2/21 Iteration 4/49 Loss 0.7561299204826355: \n",
            "Epoch 2/21 Iteration 5/49 Loss 0.7743491530418396: \n",
            "Epoch 2/21 Iteration 6/49 Loss 0.676274299621582: \n",
            "Epoch 2/21 Iteration 7/49 Loss 0.717418909072876: \n",
            "Epoch 2/21 Iteration 8/49 Loss 0.7130352258682251: \n",
            "Epoch 2/21 Iteration 9/49 Loss 0.736163854598999: \n",
            "Epoch 2/21 Iteration 10/49 Loss 0.7067115902900696: \n",
            "Epoch 2/21 Iteration 11/49 Loss 0.6843987703323364: \n",
            "Epoch 2/21 Iteration 12/49 Loss 0.7131091952323914: \n",
            "Epoch 2/21 Iteration 13/49 Loss 0.7174745798110962: \n",
            "Epoch 2/21 Iteration 14/49 Loss 0.6993507742881775: \n",
            "Epoch 2/21 Iteration 15/49 Loss 0.66960209608078: \n",
            "Epoch 2/21 Iteration 16/49 Loss 0.7359967231750488: \n",
            "Epoch 2/21 Iteration 17/49 Loss 0.6743589639663696: \n",
            "Epoch 2/21 Iteration 18/49 Loss 0.7147307395935059: \n",
            "Epoch 2/21 Iteration 19/49 Loss 0.7222887277603149: \n",
            "Epoch 2/21 Iteration 20/49 Loss 0.7190171480178833: \n",
            "Epoch 2/21 Iteration 21/49 Loss 0.6808874607086182: \n",
            "Epoch 2/21 Iteration 22/49 Loss 0.7147007584571838: \n",
            "Epoch 2/21 Iteration 23/49 Loss 0.6864649057388306: \n",
            "Epoch 2/21 Iteration 24/49 Loss 0.7573526501655579: \n",
            "Epoch 2/21 Iteration 25/49 Loss 0.7045196890830994: \n",
            "Epoch 2/21 Iteration 26/49 Loss 0.6825394630432129: \n",
            "Epoch 2/21 Iteration 27/49 Loss 0.6971071362495422: \n",
            "Epoch 2/21 Iteration 28/49 Loss 0.6419112086296082: \n",
            "Epoch 2/21 Iteration 29/49 Loss 0.730710506439209: \n",
            "Epoch 2/21 Iteration 30/49 Loss 0.6665905714035034: \n",
            "Epoch 2/21 Iteration 31/49 Loss 0.6446108818054199: \n",
            "Epoch 2/21 Iteration 32/49 Loss 0.6916543245315552: \n",
            "Epoch 2/21 Iteration 33/49 Loss 0.7248644828796387: \n",
            "Epoch 2/21 Iteration 34/49 Loss 0.6732708215713501: \n",
            "Epoch 2/21 Iteration 35/49 Loss 0.6519116759300232: \n",
            "Epoch 2/21 Iteration 36/49 Loss 0.6375089883804321: \n",
            "Epoch 2/21 Iteration 37/49 Loss 0.6846988201141357: \n",
            "Epoch 2/21 Iteration 38/49 Loss 0.6549533009529114: \n",
            "Epoch 2/21 Iteration 39/49 Loss 0.6537308692932129: \n",
            "Epoch 2/21 Iteration 40/49 Loss 0.6227751970291138: \n",
            "Epoch 2/21 Iteration 41/49 Loss 0.6522595286369324: \n",
            "Epoch 2/21 Iteration 42/49 Loss 0.6649928092956543: \n",
            "Epoch 2/21 Iteration 43/49 Loss 0.6405285596847534: \n",
            "Epoch 2/21 Iteration 44/49 Loss 0.6733497381210327: \n",
            "Epoch 2/21 Iteration 45/49 Loss 0.6995481252670288: \n",
            "Epoch 2/21 Iteration 46/49 Loss 0.7117354273796082: \n",
            "Epoch 2/21 Iteration 47/49 Loss 0.6531342267990112: \n",
            "Epoch 2/21 Iteration 48/49 Loss 0.6767892837524414: \n",
            "Epoch 3/21 Iteration 0/49 Loss 0.6612645387649536: \n",
            "Epoch 3/21 Iteration 1/49 Loss 0.6814455986022949: \n",
            "Epoch 3/21 Iteration 2/49 Loss 0.6441295146942139: \n",
            "Epoch 3/21 Iteration 3/49 Loss 0.659358024597168: \n",
            "Epoch 3/21 Iteration 4/49 Loss 0.6896004676818848: \n",
            "Epoch 3/21 Iteration 5/49 Loss 0.7033163905143738: \n",
            "Epoch 3/21 Iteration 6/49 Loss 0.609797477722168: \n",
            "Epoch 3/21 Iteration 7/49 Loss 0.6526021361351013: \n",
            "Epoch 3/21 Iteration 8/49 Loss 0.6423438787460327: \n",
            "Epoch 3/21 Iteration 9/49 Loss 0.6623749732971191: \n",
            "Epoch 3/21 Iteration 10/49 Loss 0.6440455913543701: \n",
            "Epoch 3/21 Iteration 11/49 Loss 0.6181512475013733: \n",
            "Epoch 3/21 Iteration 12/49 Loss 0.6449310779571533: \n",
            "Epoch 3/21 Iteration 13/49 Loss 0.6491413116455078: \n",
            "Epoch 3/21 Iteration 14/49 Loss 0.63504958152771: \n",
            "Epoch 3/21 Iteration 15/49 Loss 0.6121220588684082: \n",
            "Epoch 3/21 Iteration 16/49 Loss 0.6724809408187866: \n",
            "Epoch 3/21 Iteration 17/49 Loss 0.6169105172157288: \n",
            "Epoch 3/21 Iteration 18/49 Loss 0.6556462645530701: \n",
            "Epoch 3/21 Iteration 19/49 Loss 0.6637222170829773: \n",
            "Epoch 3/21 Iteration 20/49 Loss 0.669196605682373: \n",
            "Epoch 3/21 Iteration 21/49 Loss 0.6167762279510498: \n",
            "Epoch 3/21 Iteration 22/49 Loss 0.6619549989700317: \n",
            "Epoch 3/21 Iteration 23/49 Loss 0.6305142045021057: \n",
            "Epoch 3/21 Iteration 24/49 Loss 0.7046967148780823: \n",
            "Epoch 3/21 Iteration 25/49 Loss 0.6522775888442993: \n",
            "Epoch 3/21 Iteration 26/49 Loss 0.6242384314537048: \n",
            "Epoch 3/21 Iteration 27/49 Loss 0.6448637247085571: \n",
            "Epoch 3/21 Iteration 28/49 Loss 0.5896981954574585: \n",
            "Epoch 3/21 Iteration 29/49 Loss 0.6764154434204102: \n",
            "Epoch 3/21 Iteration 30/49 Loss 0.6178120374679565: \n",
            "Epoch 3/21 Iteration 31/49 Loss 0.5876120328903198: \n",
            "Epoch 3/21 Iteration 32/49 Loss 0.6463449001312256: \n",
            "Epoch 3/21 Iteration 33/49 Loss 0.6717901229858398: \n",
            "Epoch 3/21 Iteration 34/49 Loss 0.6208622455596924: \n",
            "Epoch 3/21 Iteration 35/49 Loss 0.6021389961242676: \n",
            "Epoch 3/21 Iteration 36/49 Loss 0.5869743824005127: \n",
            "Epoch 3/21 Iteration 37/49 Loss 0.6369043588638306: \n",
            "Epoch 3/21 Iteration 38/49 Loss 0.60407954454422: \n",
            "Epoch 3/21 Iteration 39/49 Loss 0.6091469526290894: \n",
            "Epoch 3/21 Iteration 40/49 Loss 0.5717140436172485: \n",
            "Epoch 3/21 Iteration 41/49 Loss 0.609581708908081: \n",
            "Epoch 3/21 Iteration 42/49 Loss 0.616819441318512: \n",
            "Epoch 3/21 Iteration 43/49 Loss 0.5914791226387024: \n",
            "Epoch 3/21 Iteration 44/49 Loss 0.6283689737319946: \n",
            "Epoch 3/21 Iteration 45/49 Loss 0.6538759469985962: \n",
            "Epoch 3/21 Iteration 46/49 Loss 0.667820930480957: \n",
            "Epoch 3/21 Iteration 47/49 Loss 0.608013927936554: \n",
            "Epoch 3/21 Iteration 48/49 Loss 0.629716694355011: \n",
            "Epoch 4/21 Iteration 0/49 Loss 0.6182270050048828: \n",
            "Epoch 4/21 Iteration 1/49 Loss 0.6396897435188293: \n",
            "Epoch 4/21 Iteration 2/49 Loss 0.5996606349945068: \n",
            "Epoch 4/21 Iteration 3/49 Loss 0.6180976629257202: \n",
            "Epoch 4/21 Iteration 4/49 Loss 0.6484636068344116: \n",
            "Epoch 4/21 Iteration 5/49 Loss 0.659629225730896: \n",
            "Epoch 4/21 Iteration 6/49 Loss 0.5697047114372253: \n",
            "Epoch 4/21 Iteration 7/49 Loss 0.6133605241775513: \n",
            "Epoch 4/21 Iteration 8/49 Loss 0.5983458757400513: \n",
            "Epoch 4/21 Iteration 9/49 Loss 0.616676926612854: \n",
            "Epoch 4/21 Iteration 10/49 Loss 0.6055929660797119: \n",
            "Epoch 4/21 Iteration 11/49 Loss 0.577436089515686: \n",
            "Epoch 4/21 Iteration 12/49 Loss 0.6016077995300293: \n",
            "Epoch 4/21 Iteration 13/49 Loss 0.6058688163757324: \n",
            "Epoch 4/21 Iteration 14/49 Loss 0.5956673622131348: \n",
            "Epoch 4/21 Iteration 15/49 Loss 0.5769186019897461: \n",
            "Epoch 4/21 Iteration 16/49 Loss 0.6335786581039429: \n",
            "Epoch 4/21 Iteration 17/49 Loss 0.5811984539031982: \n",
            "Epoch 4/21 Iteration 18/49 Loss 0.6184626221656799: \n",
            "Epoch 4/21 Iteration 19/49 Loss 0.6266424059867859: \n",
            "Epoch 4/21 Iteration 20/49 Loss 0.6396775245666504: \n",
            "Epoch 4/21 Iteration 21/49 Loss 0.5765575766563416: \n",
            "Epoch 4/21 Iteration 22/49 Loss 0.6283302307128906: \n",
            "Epoch 4/21 Iteration 23/49 Loss 0.5936652421951294: \n",
            "Epoch 4/21 Iteration 24/49 Loss 0.6707478165626526: \n",
            "Epoch 4/21 Iteration 25/49 Loss 0.6181637048721313: \n",
            "Epoch 4/21 Iteration 26/49 Loss 0.588430643081665: \n",
            "Epoch 4/21 Iteration 27/49 Loss 0.6096696853637695: \n",
            "Epoch 4/21 Iteration 28/49 Loss 0.555149495601654: \n",
            "Epoch 4/21 Iteration 29/49 Loss 0.6390026211738586: \n",
            "Epoch 4/21 Iteration 30/49 Loss 0.5854549407958984: \n",
            "Epoch 4/21 Iteration 31/49 Loss 0.5498163104057312: \n",
            "Epoch 4/21 Iteration 32/49 Loss 0.6166052222251892: \n",
            "Epoch 4/21 Iteration 33/49 Loss 0.6370891332626343: \n",
            "Epoch 4/21 Iteration 34/49 Loss 0.5869789123535156: \n",
            "Epoch 4/21 Iteration 35/49 Loss 0.5690888166427612: \n",
            "Epoch 4/21 Iteration 36/49 Loss 0.5532015562057495: \n",
            "Epoch 4/21 Iteration 37/49 Loss 0.6048871278762817: \n",
            "Epoch 4/21 Iteration 38/49 Loss 0.5696011185646057: \n",
            "Epoch 4/21 Iteration 39/49 Loss 0.5794842839241028: \n",
            "Epoch 4/21 Iteration 40/49 Loss 0.5370423793792725: \n",
            "Epoch 4/21 Iteration 41/49 Loss 0.5822190046310425: \n",
            "Epoch 4/21 Iteration 42/49 Loss 0.5840915441513062: \n",
            "Epoch 4/21 Iteration 43/49 Loss 0.5576935410499573: \n",
            "Epoch 4/21 Iteration 44/49 Loss 0.5990739464759827: \n",
            "Epoch 4/21 Iteration 45/49 Loss 0.6225970983505249: \n",
            "Epoch 4/21 Iteration 46/49 Loss 0.6371728181838989: \n",
            "Epoch 4/21 Iteration 47/49 Loss 0.5766290426254272: \n",
            "Epoch 4/21 Iteration 48/49 Loss 0.5971307754516602: \n",
            "Epoch 5/21 Iteration 0/49 Loss 0.5880978107452393: \n",
            "Epoch 5/21 Iteration 1/49 Loss 0.6105610132217407: \n",
            "Epoch 5/21 Iteration 2/49 Loss 0.5692843198776245: \n",
            "Epoch 5/21 Iteration 3/49 Loss 0.5898644328117371: \n",
            "Epoch 5/21 Iteration 4/49 Loss 0.6200352311134338: \n",
            "Epoch 5/21 Iteration 5/49 Loss 0.6291502714157104: \n",
            "Epoch 5/21 Iteration 6/49 Loss 0.5426619052886963: \n",
            "Epoch 5/21 Iteration 7/49 Loss 0.5854666233062744: \n",
            "Epoch 5/21 Iteration 8/49 Loss 0.5671917796134949: \n",
            "Epoch 5/21 Iteration 9/49 Loss 0.5853404998779297: \n",
            "Epoch 5/21 Iteration 10/49 Loss 0.5781999826431274: \n",
            "Epoch 5/21 Iteration 11/49 Loss 0.5492411255836487: \n",
            "Epoch 5/21 Iteration 12/49 Loss 0.571230947971344: \n",
            "Epoch 5/21 Iteration 13/49 Loss 0.5749255418777466: \n",
            "Epoch 5/21 Iteration 14/49 Loss 0.5687984228134155: \n",
            "Epoch 5/21 Iteration 15/49 Loss 0.5520992875099182: \n",
            "Epoch 5/21 Iteration 16/49 Loss 0.6056581735610962: \n",
            "Epoch 5/21 Iteration 17/49 Loss 0.5554134249687195: \n",
            "Epoch 5/21 Iteration 18/49 Loss 0.5910457372665405: \n",
            "Epoch 5/21 Iteration 19/49 Loss 0.6000779867172241: \n",
            "Epoch 5/21 Iteration 20/49 Loss 0.6189314126968384: \n",
            "Epoch 5/21 Iteration 21/49 Loss 0.547841489315033: \n",
            "Epoch 5/21 Iteration 22/49 Loss 0.6039059162139893: \n",
            "Epoch 5/21 Iteration 23/49 Loss 0.5672507286071777: \n",
            "Epoch 5/21 Iteration 24/49 Loss 0.6467921137809753: \n",
            "Epoch 5/21 Iteration 25/49 Loss 0.5934759378433228: \n",
            "Epoch 5/21 Iteration 26/49 Loss 0.5635424852371216: \n",
            "Epoch 5/21 Iteration 27/49 Loss 0.583505392074585: \n",
            "Epoch 5/21 Iteration 28/49 Loss 0.530215859413147: \n",
            "Epoch 5/21 Iteration 29/49 Loss 0.6119030117988586: \n",
            "Epoch 5/21 Iteration 30/49 Loss 0.5617012977600098: \n",
            "Epoch 5/21 Iteration 31/49 Loss 0.5225644111633301: \n",
            "Epoch 5/21 Iteration 32/49 Loss 0.5951626300811768: \n",
            "Epoch 5/21 Iteration 33/49 Loss 0.6119619607925415: \n",
            "Epoch 5/21 Iteration 34/49 Loss 0.5618870258331299: \n",
            "Epoch 5/21 Iteration 35/49 Loss 0.5448378324508667: \n",
            "Epoch 5/21 Iteration 36/49 Loss 0.5281164050102234: \n",
            "Epoch 5/21 Iteration 37/49 Loss 0.582378089427948: \n",
            "Epoch 5/21 Iteration 38/49 Loss 0.5440929532051086: \n",
            "Epoch 5/21 Iteration 39/49 Loss 0.5575582385063171: \n",
            "Epoch 5/21 Iteration 40/49 Loss 0.5113489627838135: \n",
            "Epoch 5/21 Iteration 41/49 Loss 0.5623760223388672: \n",
            "Epoch 5/21 Iteration 42/49 Loss 0.5594499111175537: \n",
            "Epoch 5/21 Iteration 43/49 Loss 0.532882809638977: \n",
            "Epoch 5/21 Iteration 44/49 Loss 0.5783776044845581: \n",
            "Epoch 5/21 Iteration 45/49 Loss 0.5995724201202393: \n",
            "Epoch 5/21 Iteration 46/49 Loss 0.6143851280212402: \n",
            "Epoch 5/21 Iteration 47/49 Loss 0.5530186295509338: \n",
            "Epoch 5/21 Iteration 48/49 Loss 0.572527289390564: \n",
            "Epoch 6/21 Iteration 0/49 Loss 0.5653987526893616: \n",
            "Epoch 6/21 Iteration 1/49 Loss 0.5887065529823303: \n",
            "Epoch 6/21 Iteration 2/49 Loss 0.5468300580978394: \n",
            "Epoch 6/21 Iteration 3/49 Loss 0.56910240650177: \n",
            "Epoch 6/21 Iteration 4/49 Loss 0.5981965065002441: \n",
            "Epoch 6/21 Iteration 5/49 Loss 0.6061388254165649: \n",
            "Epoch 6/21 Iteration 6/49 Loss 0.5231493711471558: \n",
            "Epoch 6/21 Iteration 7/49 Loss 0.5639473795890808: \n",
            "Epoch 6/21 Iteration 8/49 Loss 0.5435046553611755: \n",
            "Epoch 6/21 Iteration 9/49 Loss 0.5619482398033142: \n",
            "Epoch 6/21 Iteration 10/49 Loss 0.5570014715194702: \n",
            "Epoch 6/21 Iteration 11/49 Loss 0.5284271240234375: \n",
            "Epoch 6/21 Iteration 12/49 Loss 0.5481694936752319: \n",
            "Epoch 6/21 Iteration 13/49 Loss 0.551016092300415: \n",
            "Epoch 6/21 Iteration 14/49 Loss 0.547953724861145: \n",
            "Epoch 6/21 Iteration 15/49 Loss 0.5335471630096436: \n",
            "Epoch 6/21 Iteration 16/49 Loss 0.5845432877540588: \n",
            "Epoch 6/21 Iteration 17/49 Loss 0.5362564325332642: \n",
            "Epoch 6/21 Iteration 18/49 Loss 0.5707772374153137: \n",
            "Epoch 6/21 Iteration 19/49 Loss 0.5813297033309937: \n",
            "Epoch 6/21 Iteration 20/49 Loss 0.6048047542572021: \n",
            "Epoch 6/21 Iteration 21/49 Loss 0.5264273285865784: \n",
            "Epoch 6/21 Iteration 22/49 Loss 0.5846841931343079: \n",
            "Epoch 6/21 Iteration 23/49 Loss 0.5468252897262573: \n",
            "Epoch 6/21 Iteration 24/49 Loss 0.6284428834915161: \n",
            "Epoch 6/21 Iteration 25/49 Loss 0.5739445686340332: \n",
            "Epoch 6/21 Iteration 26/49 Loss 0.5442065596580505: \n",
            "Epoch 6/21 Iteration 27/49 Loss 0.5630677938461304: \n",
            "Epoch 6/21 Iteration 28/49 Loss 0.5110922455787659: \n",
            "Epoch 6/21 Iteration 29/49 Loss 0.5909790992736816: \n",
            "Epoch 6/21 Iteration 30/49 Loss 0.54290771484375: \n",
            "Epoch 6/21 Iteration 31/49 Loss 0.5018594264984131: \n",
            "Epoch 6/21 Iteration 32/49 Loss 0.578790545463562: \n",
            "Epoch 6/21 Iteration 33/49 Loss 0.5922088623046875: \n",
            "Epoch 6/21 Iteration 34/49 Loss 0.5424614548683167: \n",
            "Epoch 6/21 Iteration 35/49 Loss 0.5258603096008301: \n",
            "Epoch 6/21 Iteration 36/49 Loss 0.5083686113357544: \n",
            "Epoch 6/21 Iteration 37/49 Loss 0.5651600360870361: \n",
            "Epoch 6/21 Iteration 38/49 Loss 0.5238943696022034: \n",
            "Epoch 6/21 Iteration 39/49 Loss 0.5403093695640564: \n",
            "Epoch 6/21 Iteration 40/49 Loss 0.49153417348861694: \n",
            "Epoch 6/21 Iteration 41/49 Loss 0.5476484298706055: \n",
            "Epoch 6/21 Iteration 42/49 Loss 0.5402026176452637: \n",
            "Epoch 6/21 Iteration 43/49 Loss 0.5134726762771606: \n",
            "Epoch 6/21 Iteration 44/49 Loss 0.5618161559104919: \n",
            "Epoch 6/21 Iteration 45/49 Loss 0.5812519788742065: \n",
            "Epoch 6/21 Iteration 46/49 Loss 0.5957819223403931: \n",
            "Epoch 6/21 Iteration 47/49 Loss 0.534431517124176: \n",
            "Epoch 6/21 Iteration 48/49 Loss 0.553005576133728: \n",
            "Epoch 7/21 Iteration 0/49 Loss 0.5476604104042053: \n",
            "Epoch 7/21 Iteration 1/49 Loss 0.5710872411727905: \n",
            "Epoch 7/21 Iteration 2/49 Loss 0.529094934463501: \n",
            "Epoch 7/21 Iteration 3/49 Loss 0.5525730848312378: \n",
            "Epoch 7/21 Iteration 4/49 Loss 0.5809313058853149: \n",
            "Epoch 7/21 Iteration 5/49 Loss 0.5873209238052368: \n",
            "Epoch 7/21 Iteration 6/49 Loss 0.5078760981559753: \n",
            "Epoch 7/21 Iteration 7/49 Loss 0.5468531250953674: \n",
            "Epoch 7/21 Iteration 8/49 Loss 0.5249192714691162: \n",
            "Epoch 7/21 Iteration 9/49 Loss 0.5440850257873535: \n",
            "Epoch 7/21 Iteration 10/49 Loss 0.5400863885879517: \n",
            "Epoch 7/21 Iteration 11/49 Loss 0.5118347406387329: \n",
            "Epoch 7/21 Iteration 12/49 Loss 0.5299153327941895: \n",
            "Epoch 7/21 Iteration 13/49 Loss 0.5313782691955566: \n",
            "Epoch 7/21 Iteration 14/49 Loss 0.531391441822052: \n",
            "Epoch 7/21 Iteration 15/49 Loss 0.5187558531761169: \n",
            "Epoch 7/21 Iteration 16/49 Loss 0.5678626298904419: \n",
            "Epoch 7/21 Iteration 17/49 Loss 0.5206471681594849: \n",
            "Epoch 7/21 Iteration 18/49 Loss 0.5540008544921875: \n",
            "Epoch 7/21 Iteration 19/49 Loss 0.5662933588027954: \n",
            "Epoch 7/21 Iteration 20/49 Loss 0.5936151742935181: \n",
            "Epoch 7/21 Iteration 21/49 Loss 0.5092731714248657: \n",
            "Epoch 7/21 Iteration 22/49 Loss 0.5690988302230835: \n",
            "Epoch 7/21 Iteration 23/49 Loss 0.5307830572128296: \n",
            "Epoch 7/21 Iteration 24/49 Loss 0.613743245601654: \n",
            "Epoch 7/21 Iteration 25/49 Loss 0.5575587153434753: \n",
            "Epoch 7/21 Iteration 26/49 Loss 0.5287476778030396: \n",
            "Epoch 7/21 Iteration 27/49 Loss 0.5465689301490784: \n",
            "Epoch 7/21 Iteration 28/49 Loss 0.49533024430274963: \n",
            "Epoch 7/21 Iteration 29/49 Loss 0.573881983757019: \n",
            "Epoch 7/21 Iteration 30/49 Loss 0.5275789499282837: \n",
            "Epoch 7/21 Iteration 31/49 Loss 0.4852229058742523: \n",
            "Epoch 7/21 Iteration 32/49 Loss 0.5650401711463928: \n",
            "Epoch 7/21 Iteration 33/49 Loss 0.5757468342781067: \n",
            "Epoch 7/21 Iteration 34/49 Loss 0.5268467664718628: \n",
            "Epoch 7/21 Iteration 35/49 Loss 0.5107043981552124: \n",
            "Epoch 7/21 Iteration 36/49 Loss 0.49235230684280396: \n",
            "Epoch 7/21 Iteration 37/49 Loss 0.5513030290603638: \n",
            "Epoch 7/21 Iteration 38/49 Loss 0.5076698064804077: \n",
            "Epoch 7/21 Iteration 39/49 Loss 0.5268136262893677: \n",
            "Epoch 7/21 Iteration 40/49 Loss 0.4755065441131592: \n",
            "Epoch 7/21 Iteration 41/49 Loss 0.5358868837356567: \n",
            "Epoch 7/21 Iteration 42/49 Loss 0.5250952243804932: \n",
            "Epoch 7/21 Iteration 43/49 Loss 0.49778175354003906: \n",
            "Epoch 7/21 Iteration 44/49 Loss 0.5483134388923645: \n",
            "Epoch 7/21 Iteration 45/49 Loss 0.5667216777801514: \n",
            "Epoch 7/21 Iteration 46/49 Loss 0.5801941752433777: \n",
            "Epoch 7/21 Iteration 47/49 Loss 0.5193195343017578: \n",
            "Epoch 7/21 Iteration 48/49 Loss 0.5370604395866394: \n",
            "Epoch 8/21 Iteration 0/49 Loss 0.5329614877700806: \n",
            "Epoch 8/21 Iteration 1/49 Loss 0.556272029876709: \n",
            "Epoch 8/21 Iteration 2/49 Loss 0.5145436525344849: \n",
            "Epoch 8/21 Iteration 3/49 Loss 0.5386043787002563: \n",
            "Epoch 8/21 Iteration 4/49 Loss 0.5667372345924377: \n",
            "Epoch 8/21 Iteration 5/49 Loss 0.5716623067855835: \n",
            "Epoch 8/21 Iteration 6/49 Loss 0.4954911470413208: \n",
            "Epoch 8/21 Iteration 7/49 Loss 0.5327874422073364: \n",
            "Epoch 8/21 Iteration 8/49 Loss 0.5092722177505493: \n",
            "Epoch 8/21 Iteration 9/49 Loss 0.5294651985168457: \n",
            "Epoch 8/21 Iteration 10/49 Loss 0.5256499648094177: \n",
            "Epoch 8/21 Iteration 11/49 Loss 0.49822723865509033: \n",
            "Epoch 8/21 Iteration 12/49 Loss 0.514845609664917: \n",
            "Epoch 8/21 Iteration 13/49 Loss 0.5150479078292847: \n",
            "Epoch 8/21 Iteration 14/49 Loss 0.5178834795951843: \n",
            "Epoch 8/21 Iteration 15/49 Loss 0.5064866542816162: \n",
            "Epoch 8/21 Iteration 16/49 Loss 0.5543423891067505: \n",
            "Epoch 8/21 Iteration 17/49 Loss 0.5077285766601562: \n",
            "Epoch 8/21 Iteration 18/49 Loss 0.5406118035316467: \n",
            "Epoch 8/21 Iteration 19/49 Loss 0.5543122291564941: \n",
            "Epoch 8/21 Iteration 20/49 Loss 0.5847821235656738: \n",
            "Epoch 8/21 Iteration 21/49 Loss 0.4948495626449585: \n",
            "Epoch 8/21 Iteration 22/49 Loss 0.5560325384140015: \n",
            "Epoch 8/21 Iteration 23/49 Loss 0.5174141526222229: \n",
            "Epoch 8/21 Iteration 24/49 Loss 0.6012039184570312: \n",
            "Epoch 8/21 Iteration 25/49 Loss 0.5438292026519775: \n",
            "Epoch 8/21 Iteration 26/49 Loss 0.516122579574585: \n",
            "Epoch 8/21 Iteration 27/49 Loss 0.5328078269958496: \n",
            "Epoch 8/21 Iteration 28/49 Loss 0.48225948214530945: \n",
            "Epoch 8/21 Iteration 29/49 Loss 0.5592014789581299: \n",
            "Epoch 8/21 Iteration 30/49 Loss 0.5145094394683838: \n",
            "Epoch 8/21 Iteration 31/49 Loss 0.47140002250671387: \n",
            "Epoch 8/21 Iteration 32/49 Loss 0.5534385442733765: \n",
            "Epoch 8/21 Iteration 33/49 Loss 0.562066376209259: \n",
            "Epoch 8/21 Iteration 34/49 Loss 0.5138849020004272: \n",
            "Epoch 8/21 Iteration 35/49 Loss 0.4984744191169739: \n",
            "Epoch 8/21 Iteration 36/49 Loss 0.4791828393936157: \n",
            "Epoch 8/21 Iteration 37/49 Loss 0.5397178530693054: \n",
            "Epoch 8/21 Iteration 38/49 Loss 0.4942246973514557: \n",
            "Epoch 8/21 Iteration 39/49 Loss 0.5151833295822144: \n",
            "Epoch 8/21 Iteration 40/49 Loss 0.4622000455856323: \n",
            "Epoch 8/21 Iteration 41/49 Loss 0.5256311893463135: \n",
            "Epoch 8/21 Iteration 42/49 Loss 0.5122568607330322: \n",
            "Epoch 8/21 Iteration 43/49 Loss 0.4841100573539734: \n",
            "Epoch 8/21 Iteration 44/49 Loss 0.5368465781211853: \n",
            "Epoch 8/21 Iteration 45/49 Loss 0.554076075553894: \n",
            "Epoch 8/21 Iteration 46/49 Loss 0.5672080516815186: \n",
            "Epoch 8/21 Iteration 47/49 Loss 0.5066115856170654: \n",
            "Epoch 8/21 Iteration 48/49 Loss 0.5237477421760559: \n",
            "Epoch 9/21 Iteration 0/49 Loss 0.5204047560691833: \n",
            "Epoch 9/21 Iteration 1/49 Loss 0.5439244508743286: \n",
            "Epoch 9/21 Iteration 2/49 Loss 0.5024673938751221: \n",
            "Epoch 9/21 Iteration 3/49 Loss 0.5270825028419495: \n",
            "Epoch 9/21 Iteration 4/49 Loss 0.5547975897789001: \n",
            "Epoch 9/21 Iteration 5/49 Loss 0.5585054755210876: \n",
            "Epoch 9/21 Iteration 6/49 Loss 0.4852635860443115: \n",
            "Epoch 9/21 Iteration 7/49 Loss 0.520871639251709: \n",
            "Epoch 9/21 Iteration 8/49 Loss 0.4960782825946808: \n",
            "Epoch 9/21 Iteration 9/49 Loss 0.5168097019195557: \n",
            "Epoch 9/21 Iteration 10/49 Loss 0.5138656497001648: \n",
            "Epoch 9/21 Iteration 11/49 Loss 0.48677539825439453: \n",
            "Epoch 9/21 Iteration 12/49 Loss 0.5017225742340088: \n",
            "Epoch 9/21 Iteration 13/49 Loss 0.5010671615600586: \n",
            "Epoch 9/21 Iteration 14/49 Loss 0.5065945386886597: \n",
            "Epoch 9/21 Iteration 15/49 Loss 0.49593496322631836: \n",
            "Epoch 9/21 Iteration 16/49 Loss 0.5428130030632019: \n",
            "Epoch 9/21 Iteration 17/49 Loss 0.49690183997154236: \n",
            "Epoch 9/21 Iteration 18/49 Loss 0.529091477394104: \n",
            "Epoch 9/21 Iteration 19/49 Loss 0.544031023979187: \n",
            "Epoch 9/21 Iteration 20/49 Loss 0.5773441195487976: \n",
            "Epoch 9/21 Iteration 21/49 Loss 0.4824620187282562: \n",
            "Epoch 9/21 Iteration 22/49 Loss 0.5446478724479675: \n",
            "Epoch 9/21 Iteration 23/49 Loss 0.5060484409332275: \n",
            "Epoch 9/21 Iteration 24/49 Loss 0.5905767679214478: \n",
            "Epoch 9/21 Iteration 25/49 Loss 0.532066822052002: \n",
            "Epoch 9/21 Iteration 26/49 Loss 0.5052210092544556: \n",
            "Epoch 9/21 Iteration 27/49 Loss 0.5209416151046753: \n",
            "Epoch 9/21 Iteration 28/49 Loss 0.47108742594718933: \n",
            "Epoch 9/21 Iteration 29/49 Loss 0.546508252620697: \n",
            "Epoch 9/21 Iteration 30/49 Loss 0.5033862590789795: \n",
            "Epoch 9/21 Iteration 31/49 Loss 0.45976322889328003: \n",
            "Epoch 9/21 Iteration 32/49 Loss 0.5435968637466431: \n",
            "Epoch 9/21 Iteration 33/49 Loss 0.5503436326980591: \n",
            "Epoch 9/21 Iteration 34/49 Loss 0.5028425455093384: \n",
            "Epoch 9/21 Iteration 35/49 Loss 0.48829033970832825: \n",
            "Epoch 9/21 Iteration 36/49 Loss 0.46804577112197876: \n",
            "Epoch 9/21 Iteration 37/49 Loss 0.5300290584564209: \n",
            "Epoch 9/21 Iteration 38/49 Loss 0.48312970995903015: \n",
            "Epoch 9/21 Iteration 39/49 Loss 0.5052438974380493: \n",
            "Epoch 9/21 Iteration 40/49 Loss 0.4508681297302246: \n",
            "Epoch 9/21 Iteration 41/49 Loss 0.5169270634651184: \n",
            "Epoch 9/21 Iteration 42/49 Loss 0.5009515285491943: \n",
            "Epoch 9/21 Iteration 43/49 Loss 0.47253912687301636: \n",
            "Epoch 9/21 Iteration 44/49 Loss 0.5268880128860474: \n",
            "Epoch 9/21 Iteration 45/49 Loss 0.5432958602905273: \n",
            "Epoch 9/21 Iteration 46/49 Loss 0.5562106370925903: \n",
            "Epoch 9/21 Iteration 47/49 Loss 0.49562233686447144: \n",
            "Epoch 9/21 Iteration 48/49 Loss 0.5121058225631714: \n",
            "Epoch 10/21 Iteration 0/49 Loss 0.5097330808639526: \n",
            "Epoch 10/21 Iteration 1/49 Loss 0.5331549644470215: \n",
            "Epoch 10/21 Iteration 2/49 Loss 0.4921632409095764: \n",
            "Epoch 10/21 Iteration 3/49 Loss 0.5170254707336426: \n",
            "Epoch 10/21 Iteration 4/49 Loss 0.5444334149360657: \n",
            "Epoch 10/21 Iteration 5/49 Loss 0.547106146812439: \n",
            "Epoch 10/21 Iteration 6/49 Loss 0.47678518295288086: \n",
            "Epoch 10/21 Iteration 7/49 Loss 0.5106615424156189: \n",
            "Epoch 10/21 Iteration 8/49 Loss 0.48495814204216003: \n",
            "Epoch 10/21 Iteration 9/49 Loss 0.5060237050056458: \n",
            "Epoch 10/21 Iteration 10/49 Loss 0.503670334815979: \n",
            "Epoch 10/21 Iteration 11/49 Loss 0.47672560811042786: \n",
            "Epoch 10/21 Iteration 12/49 Loss 0.49014514684677124: \n",
            "Epoch 10/21 Iteration 13/49 Loss 0.48886626958847046: \n",
            "Epoch 10/21 Iteration 14/49 Loss 0.497092604637146: \n",
            "Epoch 10/21 Iteration 15/49 Loss 0.4868905544281006: \n",
            "Epoch 10/21 Iteration 16/49 Loss 0.5327555537223816: \n",
            "Epoch 10/21 Iteration 17/49 Loss 0.4876376986503601: \n",
            "Epoch 10/21 Iteration 18/49 Loss 0.5190336108207703: \n",
            "Epoch 10/21 Iteration 19/49 Loss 0.5353385210037231: \n",
            "Epoch 10/21 Iteration 20/49 Loss 0.5709888935089111: \n",
            "Epoch 10/21 Iteration 21/49 Loss 0.4720211625099182: \n",
            "Epoch 10/21 Iteration 22/49 Loss 0.5346420407295227: \n",
            "Epoch 10/21 Iteration 23/49 Loss 0.49610477685928345: \n",
            "Epoch 10/21 Iteration 24/49 Loss 0.5813888907432556: \n",
            "Epoch 10/21 Iteration 25/49 Loss 0.5218906998634338: \n",
            "Epoch 10/21 Iteration 26/49 Loss 0.4958198070526123: \n",
            "Epoch 10/21 Iteration 27/49 Loss 0.5105335712432861: \n",
            "Epoch 10/21 Iteration 28/49 Loss 0.46167755126953125: \n",
            "Epoch 10/21 Iteration 29/49 Loss 0.5353207588195801: \n",
            "Epoch 10/21 Iteration 30/49 Loss 0.49340230226516724: \n",
            "Epoch 10/21 Iteration 31/49 Loss 0.44988059997558594: \n",
            "Epoch 10/21 Iteration 32/49 Loss 0.5348066091537476: \n",
            "Epoch 10/21 Iteration 33/49 Loss 0.5397270321846008: \n",
            "Epoch 10/21 Iteration 34/49 Loss 0.4930400550365448: \n",
            "Epoch 10/21 Iteration 35/49 Loss 0.4795454740524292: \n",
            "Epoch 10/21 Iteration 36/49 Loss 0.45829451084136963: \n",
            "Epoch 10/21 Iteration 37/49 Loss 0.5214859247207642: \n",
            "Epoch 10/21 Iteration 38/49 Loss 0.47343212366104126: \n",
            "Epoch 10/21 Iteration 39/49 Loss 0.4963802099227905: \n",
            "Epoch 10/21 Iteration 40/49 Loss 0.4411146640777588: \n",
            "Epoch 10/21 Iteration 41/49 Loss 0.5097426772117615: \n",
            "Epoch 10/21 Iteration 42/49 Loss 0.49116063117980957: \n",
            "Epoch 10/21 Iteration 43/49 Loss 0.4620022177696228: \n",
            "Epoch 10/21 Iteration 44/49 Loss 0.5181201696395874: \n",
            "Epoch 10/21 Iteration 45/49 Loss 0.5336729884147644: \n",
            "Epoch 10/21 Iteration 46/49 Loss 0.5465667247772217: \n",
            "Epoch 10/21 Iteration 47/49 Loss 0.48607832193374634: \n",
            "Epoch 10/21 Iteration 48/49 Loss 0.5014665722846985: \n",
            "Epoch 11/21 Iteration 0/49 Loss 0.5001918077468872: \n",
            "Epoch 11/21 Iteration 1/49 Loss 0.5234017372131348: \n",
            "Epoch 11/21 Iteration 2/49 Loss 0.4827873706817627: \n",
            "Epoch 11/21 Iteration 3/49 Loss 0.5081769227981567: \n",
            "Epoch 11/21 Iteration 4/49 Loss 0.5351406931877136: \n",
            "Epoch 11/21 Iteration 5/49 Loss 0.5370901823043823: \n",
            "Epoch 11/21 Iteration 6/49 Loss 0.4696305990219116: \n",
            "Epoch 11/21 Iteration 7/49 Loss 0.5016074776649475: \n",
            "Epoch 11/21 Iteration 8/49 Loss 0.47490429878234863: \n",
            "Epoch 11/21 Iteration 9/49 Loss 0.49635049700737: \n",
            "Epoch 11/21 Iteration 10/49 Loss 0.4946609139442444: \n",
            "Epoch 11/21 Iteration 11/49 Loss 0.46794116497039795: \n",
            "Epoch 11/21 Iteration 12/49 Loss 0.4800720810890198: \n",
            "Epoch 11/21 Iteration 13/49 Loss 0.47793668508529663: \n",
            "Epoch 11/21 Iteration 14/49 Loss 0.488808274269104: \n",
            "Epoch 11/21 Iteration 15/49 Loss 0.47906193137168884: \n",
            "Epoch 11/21 Iteration 16/49 Loss 0.5240043997764587: \n",
            "Epoch 11/21 Iteration 17/49 Loss 0.479723185300827: \n",
            "Epoch 11/21 Iteration 18/49 Loss 0.5102392435073853: \n",
            "Epoch 11/21 Iteration 19/49 Loss 0.5282721519470215: \n",
            "Epoch 11/21 Iteration 20/49 Loss 0.5654739141464233: \n",
            "Epoch 11/21 Iteration 21/49 Loss 0.46273964643478394: \n",
            "Epoch 11/21 Iteration 22/49 Loss 0.5253061652183533: \n",
            "Epoch 11/21 Iteration 23/49 Loss 0.4874313771724701: \n",
            "Epoch 11/21 Iteration 24/49 Loss 0.5731852054595947: \n",
            "Epoch 11/21 Iteration 25/49 Loss 0.5127471089363098: \n",
            "Epoch 11/21 Iteration 26/49 Loss 0.48734158277511597: \n",
            "Epoch 11/21 Iteration 27/49 Loss 0.5013676285743713: \n",
            "Epoch 11/21 Iteration 28/49 Loss 0.45322921872138977: \n",
            "Epoch 11/21 Iteration 29/49 Loss 0.5255966186523438: \n",
            "Epoch 11/21 Iteration 30/49 Loss 0.48471033573150635: \n",
            "Epoch 11/21 Iteration 31/49 Loss 0.4410439729690552: \n",
            "Epoch 11/21 Iteration 32/49 Loss 0.5269677639007568: \n",
            "Epoch 11/21 Iteration 33/49 Loss 0.5301851630210876: \n",
            "Epoch 11/21 Iteration 34/49 Loss 0.48475182056427: \n",
            "Epoch 11/21 Iteration 35/49 Loss 0.4718594551086426: \n",
            "Epoch 11/21 Iteration 36/49 Loss 0.449889600276947: \n",
            "Epoch 11/21 Iteration 37/49 Loss 0.5137965083122253: \n",
            "Epoch 11/21 Iteration 38/49 Loss 0.46501994132995605: \n",
            "Epoch 11/21 Iteration 39/49 Loss 0.4884229302406311: \n",
            "Epoch 11/21 Iteration 40/49 Loss 0.43233251571655273: \n",
            "Epoch 11/21 Iteration 41/49 Loss 0.503170907497406: \n",
            "Epoch 11/21 Iteration 42/49 Loss 0.48263224959373474: \n",
            "Epoch 11/21 Iteration 43/49 Loss 0.4528607726097107: \n",
            "Epoch 11/21 Iteration 44/49 Loss 0.5103495121002197: \n",
            "Epoch 11/21 Iteration 45/49 Loss 0.5249941349029541: \n",
            "Epoch 11/21 Iteration 46/49 Loss 0.5381103754043579: \n",
            "Epoch 11/21 Iteration 47/49 Loss 0.47776123881340027: \n",
            "Epoch 11/21 Iteration 48/49 Loss 0.49186962842941284: \n",
            "Epoch 12/21 Iteration 0/49 Loss 0.492114394903183: \n",
            "Epoch 12/21 Iteration 1/49 Loss 0.5146937370300293: \n",
            "Epoch 12/21 Iteration 2/49 Loss 0.4744482636451721: \n",
            "Epoch 12/21 Iteration 3/49 Loss 0.5003261566162109: \n",
            "Epoch 12/21 Iteration 4/49 Loss 0.5267501473426819: \n",
            "Epoch 12/21 Iteration 5/49 Loss 0.5282024145126343: \n",
            "Epoch 12/21 Iteration 6/49 Loss 0.4631662368774414: \n",
            "Epoch 12/21 Iteration 7/49 Loss 0.4936017096042633: \n",
            "Epoch 12/21 Iteration 8/49 Loss 0.4658375382423401: \n",
            "Epoch 12/21 Iteration 9/49 Loss 0.48781412839889526: \n",
            "Epoch 12/21 Iteration 10/49 Loss 0.4864646792411804: \n",
            "Epoch 12/21 Iteration 11/49 Loss 0.4598303437232971: \n",
            "Epoch 12/21 Iteration 12/49 Loss 0.4712781608104706: \n",
            "Epoch 12/21 Iteration 13/49 Loss 0.46837201714515686: \n",
            "Epoch 12/21 Iteration 14/49 Loss 0.4813643991947174: \n",
            "Epoch 12/21 Iteration 15/49 Loss 0.47211626172065735: \n",
            "Epoch 12/21 Iteration 16/49 Loss 0.5163060426712036: \n",
            "Epoch 12/21 Iteration 17/49 Loss 0.4725134074687958: \n",
            "Epoch 12/21 Iteration 18/49 Loss 0.5025652647018433: \n",
            "Epoch 12/21 Iteration 19/49 Loss 0.5217813849449158: \n",
            "Epoch 12/21 Iteration 20/49 Loss 0.5605431795120239: \n",
            "Epoch 12/21 Iteration 21/49 Loss 0.454628586769104: \n",
            "Epoch 12/21 Iteration 22/49 Loss 0.5171191692352295: \n",
            "Epoch 12/21 Iteration 23/49 Loss 0.4796542823314667: \n",
            "Epoch 12/21 Iteration 24/49 Loss 0.5657446384429932: \n",
            "Epoch 12/21 Iteration 25/49 Loss 0.5044894218444824: \n",
            "Epoch 12/21 Iteration 26/49 Loss 0.4797127842903137: \n",
            "Epoch 12/21 Iteration 27/49 Loss 0.4933685064315796: \n",
            "Epoch 12/21 Iteration 28/49 Loss 0.4456610679626465: \n",
            "Epoch 12/21 Iteration 29/49 Loss 0.517027735710144: \n",
            "Epoch 12/21 Iteration 30/49 Loss 0.47684746980667114: \n",
            "Epoch 12/21 Iteration 31/49 Loss 0.4333658218383789: \n",
            "Epoch 12/21 Iteration 32/49 Loss 0.520133376121521: \n",
            "Epoch 12/21 Iteration 33/49 Loss 0.5217182636260986: \n",
            "Epoch 12/21 Iteration 34/49 Loss 0.4773782789707184: \n",
            "Epoch 12/21 Iteration 35/49 Loss 0.46488550305366516: \n",
            "Epoch 12/21 Iteration 36/49 Loss 0.442382276058197: \n",
            "Epoch 12/21 Iteration 37/49 Loss 0.5068650841712952: \n",
            "Epoch 12/21 Iteration 38/49 Loss 0.45770615339279175: \n",
            "Epoch 12/21 Iteration 39/49 Loss 0.4812666177749634: \n",
            "Epoch 12/21 Iteration 40/49 Loss 0.42455607652664185: \n",
            "Epoch 12/21 Iteration 41/49 Loss 0.49707862734794617: \n",
            "Epoch 12/21 Iteration 42/49 Loss 0.47521480917930603: \n",
            "Epoch 12/21 Iteration 43/49 Loss 0.44452202320098877: \n",
            "Epoch 12/21 Iteration 44/49 Loss 0.5033606290817261: \n",
            "Epoch 12/21 Iteration 45/49 Loss 0.517274796962738: \n",
            "Epoch 12/21 Iteration 46/49 Loss 0.5307058095932007: \n",
            "Epoch 12/21 Iteration 47/49 Loss 0.47029542922973633: \n",
            "Epoch 12/21 Iteration 48/49 Loss 0.4833449721336365: \n",
            "Epoch 13/21 Iteration 0/49 Loss 0.48492029309272766: \n",
            "Epoch 13/21 Iteration 1/49 Loss 0.5069370865821838: \n",
            "Epoch 13/21 Iteration 2/49 Loss 0.4670734405517578: \n",
            "Epoch 13/21 Iteration 3/49 Loss 0.49346858263015747: \n",
            "Epoch 13/21 Iteration 4/49 Loss 0.5190435647964478: \n",
            "Epoch 13/21 Iteration 5/49 Loss 0.5201987624168396: \n",
            "Epoch 13/21 Iteration 6/49 Loss 0.4574430584907532: \n",
            "Epoch 13/21 Iteration 7/49 Loss 0.4863941967487335: \n",
            "Epoch 13/21 Iteration 8/49 Loss 0.457531213760376: \n",
            "Epoch 13/21 Iteration 9/49 Loss 0.48006853461265564: \n",
            "Epoch 13/21 Iteration 10/49 Loss 0.4788099527359009: \n",
            "Epoch 13/21 Iteration 11/49 Loss 0.45266467332839966: \n",
            "Epoch 13/21 Iteration 12/49 Loss 0.463460236787796: \n",
            "Epoch 13/21 Iteration 13/49 Loss 0.4599020779132843: \n",
            "Epoch 13/21 Iteration 14/49 Loss 0.47484323382377625: \n",
            "Epoch 13/21 Iteration 15/49 Loss 0.4657168388366699: \n",
            "Epoch 13/21 Iteration 16/49 Loss 0.5092043280601501: \n",
            "Epoch 13/21 Iteration 17/49 Loss 0.4661531448364258: \n",
            "Epoch 13/21 Iteration 18/49 Loss 0.4952704906463623: \n",
            "Epoch 13/21 Iteration 19/49 Loss 0.515623927116394: \n",
            "Epoch 13/21 Iteration 20/49 Loss 0.5555156469345093: \n",
            "Epoch 13/21 Iteration 21/49 Loss 0.4475848972797394: \n",
            "Epoch 13/21 Iteration 22/49 Loss 0.5097576379776001: \n",
            "Epoch 13/21 Iteration 23/49 Loss 0.4726966321468353: \n",
            "Epoch 13/21 Iteration 24/49 Loss 0.559017539024353: \n",
            "Epoch 13/21 Iteration 25/49 Loss 0.49718400835990906: \n",
            "Epoch 13/21 Iteration 26/49 Loss 0.4728755056858063: \n",
            "Epoch 13/21 Iteration 27/49 Loss 0.48617246747016907: \n",
            "Epoch 13/21 Iteration 28/49 Loss 0.438988596200943: \n",
            "Epoch 13/21 Iteration 29/49 Loss 0.5093065500259399: \n",
            "Epoch 13/21 Iteration 30/49 Loss 0.4694713354110718: \n",
            "Epoch 13/21 Iteration 31/49 Loss 0.42629289627075195: \n",
            "Epoch 13/21 Iteration 32/49 Loss 0.5138630867004395: \n",
            "Epoch 13/21 Iteration 33/49 Loss 0.5139779448509216: \n",
            "Epoch 13/21 Iteration 34/49 Loss 0.4708552956581116: \n",
            "Epoch 13/21 Iteration 35/49 Loss 0.4586358368396759: \n",
            "Epoch 13/21 Iteration 36/49 Loss 0.4354472756385803: \n",
            "Epoch 13/21 Iteration 37/49 Loss 0.5006592869758606: \n",
            "Epoch 13/21 Iteration 38/49 Loss 0.45125555992126465: \n",
            "Epoch 13/21 Iteration 39/49 Loss 0.47474968433380127: \n",
            "Epoch 13/21 Iteration 40/49 Loss 0.41762739419937134: \n",
            "Epoch 13/21 Iteration 41/49 Loss 0.49143627285957336: \n",
            "Epoch 13/21 Iteration 42/49 Loss 0.46804818511009216: \n",
            "Epoch 13/21 Iteration 43/49 Loss 0.436593234539032: \n",
            "Epoch 13/21 Iteration 44/49 Loss 0.4967119097709656: \n",
            "Epoch 13/21 Iteration 45/49 Loss 0.5101908445358276: \n",
            "Epoch 13/21 Iteration 46/49 Loss 0.5238516330718994: \n",
            "Epoch 13/21 Iteration 47/49 Loss 0.46347296237945557: \n",
            "Epoch 13/21 Iteration 48/49 Loss 0.47562164068222046: \n",
            "Epoch 14/21 Iteration 0/49 Loss 0.4783000648021698: \n",
            "Epoch 14/21 Iteration 1/49 Loss 0.49973785877227783: \n",
            "Epoch 14/21 Iteration 2/49 Loss 0.4604908227920532: \n",
            "Epoch 14/21 Iteration 3/49 Loss 0.4871472120285034: \n",
            "Epoch 14/21 Iteration 4/49 Loss 0.5119669437408447: \n",
            "Epoch 14/21 Iteration 5/49 Loss 0.5128798484802246: \n",
            "Epoch 14/21 Iteration 6/49 Loss 0.4523710310459137: \n",
            "Epoch 14/21 Iteration 7/49 Loss 0.4800584614276886: \n",
            "Epoch 14/21 Iteration 8/49 Loss 0.45038723945617676: \n",
            "Epoch 14/21 Iteration 9/49 Loss 0.4730968177318573: \n",
            "Epoch 14/21 Iteration 10/49 Loss 0.4718325734138489: \n",
            "Epoch 14/21 Iteration 11/49 Loss 0.4462951421737671: \n",
            "Epoch 14/21 Iteration 12/49 Loss 0.4565882682800293: \n",
            "Epoch 14/21 Iteration 13/49 Loss 0.4521539807319641: \n",
            "Epoch 14/21 Iteration 14/49 Loss 0.4688186049461365: \n",
            "Epoch 14/21 Iteration 15/49 Loss 0.45986104011535645: \n",
            "Epoch 14/21 Iteration 16/49 Loss 0.5029852390289307: \n",
            "Epoch 14/21 Iteration 17/49 Loss 0.4603964388370514: \n",
            "Epoch 14/21 Iteration 18/49 Loss 0.4886760413646698: \n",
            "Epoch 14/21 Iteration 19/49 Loss 0.5098668932914734: \n",
            "Epoch 14/21 Iteration 20/49 Loss 0.5511243343353271: \n",
            "Epoch 14/21 Iteration 21/49 Loss 0.44117671251296997: \n",
            "Epoch 14/21 Iteration 22/49 Loss 0.5028793811798096: \n",
            "Epoch 14/21 Iteration 23/49 Loss 0.4662388563156128: \n",
            "Epoch 14/21 Iteration 24/49 Loss 0.5529932379722595: \n",
            "Epoch 14/21 Iteration 25/49 Loss 0.4904101490974426: \n",
            "Epoch 14/21 Iteration 26/49 Loss 0.46647363901138306: \n",
            "Epoch 14/21 Iteration 27/49 Loss 0.4795861840248108: \n",
            "Epoch 14/21 Iteration 28/49 Loss 0.4329061508178711: \n",
            "Epoch 14/21 Iteration 29/49 Loss 0.5021575093269348: \n",
            "Epoch 14/21 Iteration 30/49 Loss 0.46271955966949463: \n",
            "Epoch 14/21 Iteration 31/49 Loss 0.4197137653827667: \n",
            "Epoch 14/21 Iteration 32/49 Loss 0.5080006122589111: \n",
            "Epoch 14/21 Iteration 33/49 Loss 0.5068920254707336: \n",
            "Epoch 14/21 Iteration 34/49 Loss 0.46507465839385986: \n",
            "Epoch 14/21 Iteration 35/49 Loss 0.4525895118713379: \n",
            "Epoch 14/21 Iteration 36/49 Loss 0.42916563153266907: \n",
            "Epoch 14/21 Iteration 37/49 Loss 0.4949615001678467: \n",
            "Epoch 14/21 Iteration 38/49 Loss 0.44521647691726685: \n",
            "Epoch 14/21 Iteration 39/49 Loss 0.4688749611377716: \n",
            "Epoch 14/21 Iteration 40/49 Loss 0.4112735092639923: \n",
            "Epoch 14/21 Iteration 41/49 Loss 0.48627012968063354: \n",
            "Epoch 14/21 Iteration 42/49 Loss 0.4617893099784851: \n",
            "Epoch 14/21 Iteration 43/49 Loss 0.42948848009109497: \n",
            "Epoch 14/21 Iteration 44/49 Loss 0.49073880910873413: \n",
            "Epoch 14/21 Iteration 45/49 Loss 0.5040441155433655: \n",
            "Epoch 14/21 Iteration 46/49 Loss 0.5177099704742432: \n",
            "Epoch 14/21 Iteration 47/49 Loss 0.45717859268188477: \n",
            "Epoch 14/21 Iteration 48/49 Loss 0.46850207448005676: \n",
            "Epoch 15/21 Iteration 0/49 Loss 0.4721924364566803: \n",
            "Epoch 15/21 Iteration 1/49 Loss 0.49304646253585815: \n",
            "Epoch 15/21 Iteration 2/49 Loss 0.4544198513031006: \n",
            "Epoch 15/21 Iteration 3/49 Loss 0.48137933015823364: \n",
            "Epoch 15/21 Iteration 4/49 Loss 0.505334734916687: \n",
            "Epoch 15/21 Iteration 5/49 Loss 0.5059949159622192: \n",
            "Epoch 15/21 Iteration 6/49 Loss 0.4475718140602112: \n",
            "Epoch 15/21 Iteration 7/49 Loss 0.4742782711982727: \n",
            "Epoch 15/21 Iteration 8/49 Loss 0.4436160624027252: \n",
            "Epoch 15/21 Iteration 9/49 Loss 0.46679699420928955: \n",
            "Epoch 15/21 Iteration 10/49 Loss 0.4653334319591522: \n",
            "Epoch 15/21 Iteration 11/49 Loss 0.4403453469276428: \n",
            "Epoch 15/21 Iteration 12/49 Loss 0.45005476474761963: \n",
            "Epoch 15/21 Iteration 13/49 Loss 0.4448913335800171: \n",
            "Epoch 15/21 Iteration 14/49 Loss 0.46326369047164917: \n",
            "Epoch 15/21 Iteration 15/49 Loss 0.4542316198348999: \n",
            "Epoch 15/21 Iteration 16/49 Loss 0.4972044825553894: \n",
            "Epoch 15/21 Iteration 17/49 Loss 0.4551874101161957: \n",
            "Epoch 15/21 Iteration 18/49 Loss 0.48251035809516907: \n",
            "Epoch 15/21 Iteration 19/49 Loss 0.5044220089912415: \n",
            "Epoch 15/21 Iteration 20/49 Loss 0.5470442175865173: \n",
            "Epoch 15/21 Iteration 21/49 Loss 0.43518900871276855: \n",
            "Epoch 15/21 Iteration 22/49 Loss 0.4965980648994446: \n",
            "Epoch 15/21 Iteration 23/49 Loss 0.4601449966430664: \n",
            "Epoch 15/21 Iteration 24/49 Loss 0.5476837158203125: \n",
            "Epoch 15/21 Iteration 25/49 Loss 0.4840952754020691: \n",
            "Epoch 15/21 Iteration 26/49 Loss 0.4606838822364807: \n",
            "Epoch 15/21 Iteration 27/49 Loss 0.4732401967048645: \n",
            "Epoch 15/21 Iteration 28/49 Loss 0.42734450101852417: \n",
            "Epoch 15/21 Iteration 29/49 Loss 0.4955778121948242: \n",
            "Epoch 15/21 Iteration 30/49 Loss 0.45644932985305786: \n",
            "Epoch 15/21 Iteration 31/49 Loss 0.4135538339614868: \n",
            "Epoch 15/21 Iteration 32/49 Loss 0.5026222467422485: \n",
            "Epoch 15/21 Iteration 33/49 Loss 0.5002349615097046: \n",
            "Epoch 15/21 Iteration 34/49 Loss 0.45991867780685425: \n",
            "Epoch 15/21 Iteration 35/49 Loss 0.4471901059150696: \n",
            "Epoch 15/21 Iteration 36/49 Loss 0.4233366847038269: \n",
            "Epoch 15/21 Iteration 37/49 Loss 0.48953646421432495: \n",
            "Epoch 15/21 Iteration 38/49 Loss 0.43938398361206055: \n",
            "Epoch 15/21 Iteration 39/49 Loss 0.46339577436447144: \n",
            "Epoch 15/21 Iteration 40/49 Loss 0.4054815173149109: \n",
            "Epoch 15/21 Iteration 41/49 Loss 0.48151275515556335: \n",
            "Epoch 15/21 Iteration 42/49 Loss 0.4562631845474243: \n",
            "Epoch 15/21 Iteration 43/49 Loss 0.42297500371932983: \n",
            "Epoch 15/21 Iteration 44/49 Loss 0.48516398668289185: \n",
            "Epoch 15/21 Iteration 45/49 Loss 0.4983268678188324: \n",
            "Epoch 15/21 Iteration 46/49 Loss 0.5118695497512817: \n",
            "Epoch 15/21 Iteration 47/49 Loss 0.4511112570762634: \n",
            "Epoch 15/21 Iteration 48/49 Loss 0.4619201123714447: \n",
            "Epoch 16/21 Iteration 0/49 Loss 0.46662670373916626: \n",
            "Epoch 16/21 Iteration 1/49 Loss 0.4870927929878235: \n",
            "Epoch 16/21 Iteration 2/49 Loss 0.44910067319869995: \n",
            "Epoch 16/21 Iteration 3/49 Loss 0.4760993719100952: \n",
            "Epoch 16/21 Iteration 4/49 Loss 0.49914276599884033: \n",
            "Epoch 16/21 Iteration 5/49 Loss 0.4998469948768616: \n",
            "Epoch 16/21 Iteration 6/49 Loss 0.4431104362010956: \n",
            "Epoch 16/21 Iteration 7/49 Loss 0.46894824504852295: \n",
            "Epoch 16/21 Iteration 8/49 Loss 0.4372568130493164: \n",
            "Epoch 16/21 Iteration 9/49 Loss 0.4607841968536377: \n",
            "Epoch 16/21 Iteration 10/49 Loss 0.45924896001815796: \n",
            "Epoch 16/21 Iteration 11/49 Loss 0.4347923994064331: \n",
            "Epoch 16/21 Iteration 12/49 Loss 0.4439484477043152: \n",
            "Epoch 16/21 Iteration 13/49 Loss 0.4382573962211609: \n",
            "Epoch 16/21 Iteration 14/49 Loss 0.45801299810409546: \n",
            "Epoch 16/21 Iteration 15/49 Loss 0.4489292502403259: \n",
            "Epoch 16/21 Iteration 16/49 Loss 0.4915217459201813: \n",
            "Epoch 16/21 Iteration 17/49 Loss 0.450163871049881: \n",
            "Epoch 16/21 Iteration 18/49 Loss 0.4766443371772766: \n",
            "Epoch 16/21 Iteration 19/49 Loss 0.4992280602455139: \n",
            "Epoch 16/21 Iteration 20/49 Loss 0.5427649021148682: \n",
            "Epoch 16/21 Iteration 21/49 Loss 0.42972689867019653: \n",
            "Epoch 16/21 Iteration 22/49 Loss 0.4907131791114807: \n",
            "Epoch 16/21 Iteration 23/49 Loss 0.45468443632125854: \n",
            "Epoch 16/21 Iteration 24/49 Loss 0.5428222417831421: \n",
            "Epoch 16/21 Iteration 25/49 Loss 0.4782770574092865: \n",
            "Epoch 16/21 Iteration 26/49 Loss 0.4552920460700989: \n",
            "Epoch 16/21 Iteration 27/49 Loss 0.4674857258796692: \n",
            "Epoch 16/21 Iteration 28/49 Loss 0.42244669795036316: \n",
            "Epoch 16/21 Iteration 29/49 Loss 0.4895097315311432: \n",
            "Epoch 16/21 Iteration 30/49 Loss 0.450635701417923: \n",
            "Epoch 16/21 Iteration 31/49 Loss 0.40778785943984985: \n",
            "Epoch 16/21 Iteration 32/49 Loss 0.4978008568286896: \n",
            "Epoch 16/21 Iteration 33/49 Loss 0.4941985011100769: \n",
            "Epoch 16/21 Iteration 34/49 Loss 0.45504072308540344: \n",
            "Epoch 16/21 Iteration 35/49 Loss 0.4419623613357544: \n",
            "Epoch 16/21 Iteration 36/49 Loss 0.41801831126213074: \n",
            "Epoch 16/21 Iteration 37/49 Loss 0.4842927157878876: \n",
            "Epoch 16/21 Iteration 38/49 Loss 0.43411609530448914: \n",
            "Epoch 16/21 Iteration 39/49 Loss 0.45829153060913086: \n",
            "Epoch 16/21 Iteration 40/49 Loss 0.40006697177886963: \n",
            "Epoch 16/21 Iteration 41/49 Loss 0.47716012597084045: \n",
            "Epoch 16/21 Iteration 42/49 Loss 0.45107388496398926: \n",
            "Epoch 16/21 Iteration 43/49 Loss 0.4171399176120758: \n",
            "Epoch 16/21 Iteration 44/49 Loss 0.47993290424346924: \n",
            "Epoch 16/21 Iteration 45/49 Loss 0.49299806356430054: \n",
            "Epoch 16/21 Iteration 46/49 Loss 0.5062633752822876: \n",
            "Epoch 16/21 Iteration 47/49 Loss 0.4456879496574402: \n",
            "Epoch 16/21 Iteration 48/49 Loss 0.4557322561740875: \n",
            "Epoch 17/21 Iteration 0/49 Loss 0.4614383280277252: \n",
            "Epoch 17/21 Iteration 1/49 Loss 0.4816647171974182: \n",
            "Epoch 17/21 Iteration 2/49 Loss 0.44402891397476196: \n",
            "Epoch 17/21 Iteration 3/49 Loss 0.471221387386322: \n",
            "Epoch 17/21 Iteration 4/49 Loss 0.4933584928512573: \n",
            "Epoch 17/21 Iteration 5/49 Loss 0.4939793348312378: \n",
            "Epoch 17/21 Iteration 6/49 Loss 0.4388693571090698: \n",
            "Epoch 17/21 Iteration 7/49 Loss 0.46405595541000366: \n",
            "Epoch 17/21 Iteration 8/49 Loss 0.4313575029373169: \n",
            "Epoch 17/21 Iteration 9/49 Loss 0.4554511606693268: \n",
            "Epoch 17/21 Iteration 10/49 Loss 0.4536685347557068: \n",
            "Epoch 17/21 Iteration 11/49 Loss 0.42969173192977905: \n",
            "Epoch 17/21 Iteration 12/49 Loss 0.438251793384552: \n",
            "Epoch 17/21 Iteration 13/49 Loss 0.4321252107620239: \n",
            "Epoch 17/21 Iteration 14/49 Loss 0.4530307650566101: \n",
            "Epoch 17/21 Iteration 15/49 Loss 0.44414156675338745: \n",
            "Epoch 17/21 Iteration 16/49 Loss 0.48627281188964844: \n",
            "Epoch 17/21 Iteration 17/49 Loss 0.44542357325553894: \n",
            "Epoch 17/21 Iteration 18/49 Loss 0.4711048901081085: \n",
            "Epoch 17/21 Iteration 19/49 Loss 0.49411740899086: \n",
            "Epoch 17/21 Iteration 20/49 Loss 0.5383666753768921: \n",
            "Epoch 17/21 Iteration 21/49 Loss 0.4246510863304138: \n",
            "Epoch 17/21 Iteration 22/49 Loss 0.4852626919746399: \n",
            "Epoch 17/21 Iteration 23/49 Loss 0.4494878053665161: \n",
            "Epoch 17/21 Iteration 24/49 Loss 0.5380513668060303: \n",
            "Epoch 17/21 Iteration 25/49 Loss 0.4728129506111145: \n",
            "Epoch 17/21 Iteration 26/49 Loss 0.4505005180835724: \n",
            "Epoch 17/21 Iteration 27/49 Loss 0.4620731472969055: \n",
            "Epoch 17/21 Iteration 28/49 Loss 0.41779109835624695: \n",
            "Epoch 17/21 Iteration 29/49 Loss 0.4837251901626587: \n",
            "Epoch 17/21 Iteration 30/49 Loss 0.44529253244400024: \n",
            "Epoch 17/21 Iteration 31/49 Loss 0.4024224579334259: \n",
            "Epoch 17/21 Iteration 32/49 Loss 0.4935399293899536: \n",
            "Epoch 17/21 Iteration 33/49 Loss 0.4885346293449402: \n",
            "Epoch 17/21 Iteration 34/49 Loss 0.4505223333835602: \n",
            "Epoch 17/21 Iteration 35/49 Loss 0.43738892674446106: \n",
            "Epoch 17/21 Iteration 36/49 Loss 0.4131564497947693: \n",
            "Epoch 17/21 Iteration 37/49 Loss 0.47931885719299316: \n",
            "Epoch 17/21 Iteration 38/49 Loss 0.4291582703590393: \n",
            "Epoch 17/21 Iteration 39/49 Loss 0.4536181390285492: \n",
            "Epoch 17/21 Iteration 40/49 Loss 0.39489591121673584: \n",
            "Epoch 17/21 Iteration 41/49 Loss 0.4725179076194763: \n",
            "Epoch 17/21 Iteration 42/49 Loss 0.44609275460243225: \n",
            "Epoch 17/21 Iteration 43/49 Loss 0.41156619787216187: \n",
            "Epoch 17/21 Iteration 44/49 Loss 0.4747889041900635: \n",
            "Epoch 17/21 Iteration 45/49 Loss 0.4879569113254547: \n",
            "Epoch 17/21 Iteration 46/49 Loss 0.501128613948822: \n",
            "Epoch 17/21 Iteration 47/49 Loss 0.4408729672431946: \n",
            "Epoch 17/21 Iteration 48/49 Loss 0.4498499929904938: \n",
            "Epoch 18/21 Iteration 0/49 Loss 0.4568710923194885: \n",
            "Epoch 18/21 Iteration 1/49 Loss 0.4766972064971924: \n",
            "Epoch 18/21 Iteration 2/49 Loss 0.43955016136169434: \n",
            "Epoch 18/21 Iteration 3/49 Loss 0.46663856506347656: \n",
            "Epoch 18/21 Iteration 4/49 Loss 0.48795127868652344: \n",
            "Epoch 18/21 Iteration 5/49 Loss 0.48885276913642883: \n",
            "Epoch 18/21 Iteration 6/49 Loss 0.4347746968269348: \n",
            "Epoch 18/21 Iteration 7/49 Loss 0.459151029586792: \n",
            "Epoch 18/21 Iteration 8/49 Loss 0.42593684792518616: \n",
            "Epoch 18/21 Iteration 9/49 Loss 0.45026642084121704: \n",
            "Epoch 18/21 Iteration 10/49 Loss 0.4485396146774292: \n",
            "Epoch 18/21 Iteration 11/49 Loss 0.4251348376274109: \n",
            "Epoch 18/21 Iteration 12/49 Loss 0.43274232745170593: \n",
            "Epoch 18/21 Iteration 13/49 Loss 0.4266149401664734: \n",
            "Epoch 18/21 Iteration 14/49 Loss 0.4485113024711609: \n",
            "Epoch 18/21 Iteration 15/49 Loss 0.4398673474788666: \n",
            "Epoch 18/21 Iteration 16/49 Loss 0.4816977381706238: \n",
            "Epoch 18/21 Iteration 17/49 Loss 0.4410652220249176: \n",
            "Epoch 18/21 Iteration 18/49 Loss 0.4662461280822754: \n",
            "Epoch 18/21 Iteration 19/49 Loss 0.4898781180381775: \n",
            "Epoch 18/21 Iteration 20/49 Loss 0.5348230600357056: \n",
            "Epoch 18/21 Iteration 21/49 Loss 0.4198186993598938: \n",
            "Epoch 18/21 Iteration 22/49 Loss 0.4799588918685913: \n",
            "Epoch 18/21 Iteration 23/49 Loss 0.4446185827255249: \n",
            "Epoch 18/21 Iteration 24/49 Loss 0.533689022064209: \n",
            "Epoch 18/21 Iteration 25/49 Loss 0.4677530527114868: \n",
            "Epoch 18/21 Iteration 26/49 Loss 0.44595468044281006: \n",
            "Epoch 18/21 Iteration 27/49 Loss 0.4571065604686737: \n",
            "Epoch 18/21 Iteration 28/49 Loss 0.41352611780166626: \n",
            "Epoch 18/21 Iteration 29/49 Loss 0.47838348150253296: \n",
            "Epoch 18/21 Iteration 30/49 Loss 0.44042789936065674: \n",
            "Epoch 18/21 Iteration 31/49 Loss 0.39757847785949707: \n",
            "Epoch 18/21 Iteration 32/49 Loss 0.4895961284637451: \n",
            "Epoch 18/21 Iteration 33/49 Loss 0.48322948813438416: \n",
            "Epoch 18/21 Iteration 34/49 Loss 0.44627684354782104: \n",
            "Epoch 18/21 Iteration 35/49 Loss 0.43297243118286133: \n",
            "Epoch 18/21 Iteration 36/49 Loss 0.4085618853569031: \n",
            "Epoch 18/21 Iteration 37/49 Loss 0.4746534526348114: \n",
            "Epoch 18/21 Iteration 38/49 Loss 0.4245576560497284: \n",
            "Epoch 18/21 Iteration 39/49 Loss 0.4491852819919586: \n",
            "Epoch 18/21 Iteration 40/49 Loss 0.38991424441337585: \n",
            "Epoch 18/21 Iteration 41/49 Loss 0.4682992398738861: \n",
            "Epoch 18/21 Iteration 42/49 Loss 0.44126540422439575: \n",
            "Epoch 18/21 Iteration 43/49 Loss 0.4064192771911621: \n",
            "Epoch 18/21 Iteration 44/49 Loss 0.46999868750572205: \n",
            "Epoch 18/21 Iteration 45/49 Loss 0.4833294153213501: \n",
            "Epoch 18/21 Iteration 46/49 Loss 0.4963228404521942: \n",
            "Epoch 18/21 Iteration 47/49 Loss 0.43630123138427734: \n",
            "Epoch 18/21 Iteration 48/49 Loss 0.44421064853668213: \n",
            "Epoch 19/21 Iteration 0/49 Loss 0.45255398750305176: \n",
            "Epoch 19/21 Iteration 1/49 Loss 0.4720050096511841: \n",
            "Epoch 19/21 Iteration 2/49 Loss 0.4353392422199249: \n",
            "Epoch 19/21 Iteration 3/49 Loss 0.46228450536727905: \n",
            "Epoch 19/21 Iteration 4/49 Loss 0.48299646377563477: \n",
            "Epoch 19/21 Iteration 5/49 Loss 0.4840765595436096: \n",
            "Epoch 19/21 Iteration 6/49 Loss 0.431006520986557: \n",
            "Epoch 19/21 Iteration 7/49 Loss 0.4545307159423828: \n",
            "Epoch 19/21 Iteration 8/49 Loss 0.4206026792526245: \n",
            "Epoch 19/21 Iteration 9/49 Loss 0.4454379975795746: \n",
            "Epoch 19/21 Iteration 10/49 Loss 0.44378790259361267: \n",
            "Epoch 19/21 Iteration 11/49 Loss 0.4208555221557617: \n",
            "Epoch 19/21 Iteration 12/49 Loss 0.4275709390640259: \n",
            "Epoch 19/21 Iteration 13/49 Loss 0.4213355481624603: \n",
            "Epoch 19/21 Iteration 14/49 Loss 0.4442884624004364: \n",
            "Epoch 19/21 Iteration 15/49 Loss 0.4357212781906128: \n",
            "Epoch 19/21 Iteration 16/49 Loss 0.47708773612976074: \n",
            "Epoch 19/21 Iteration 17/49 Loss 0.4372492730617523: \n",
            "Epoch 19/21 Iteration 18/49 Loss 0.4617489278316498: \n",
            "Epoch 19/21 Iteration 19/49 Loss 0.4862439036369324: \n",
            "Epoch 19/21 Iteration 20/49 Loss 0.5317666530609131: \n",
            "Epoch 19/21 Iteration 21/49 Loss 0.41524165868759155: \n",
            "Epoch 19/21 Iteration 22/49 Loss 0.4749189019203186: \n",
            "Epoch 19/21 Iteration 23/49 Loss 0.44014132022857666: \n",
            "Epoch 19/21 Iteration 24/49 Loss 0.5295408368110657: \n",
            "Epoch 19/21 Iteration 25/49 Loss 0.46309036016464233: \n",
            "Epoch 19/21 Iteration 26/49 Loss 0.44190701842308044: \n",
            "Epoch 19/21 Iteration 27/49 Loss 0.4524514079093933: \n",
            "Epoch 19/21 Iteration 28/49 Loss 0.40932852029800415: \n",
            "Epoch 19/21 Iteration 29/49 Loss 0.4731837511062622: \n",
            "Epoch 19/21 Iteration 30/49 Loss 0.4358963966369629: \n",
            "Epoch 19/21 Iteration 31/49 Loss 0.3930206298828125: \n",
            "Epoch 19/21 Iteration 32/49 Loss 0.48581662774086: \n",
            "Epoch 19/21 Iteration 33/49 Loss 0.47818878293037415: \n",
            "Epoch 19/21 Iteration 34/49 Loss 0.442527711391449: \n",
            "Epoch 19/21 Iteration 35/49 Loss 0.4290081858634949: \n",
            "Epoch 19/21 Iteration 36/49 Loss 0.40435928106307983: \n",
            "Epoch 19/21 Iteration 37/49 Loss 0.47040578722953796: \n",
            "Epoch 19/21 Iteration 38/49 Loss 0.4202898144721985: \n",
            "Epoch 19/21 Iteration 39/49 Loss 0.4449365735054016: \n",
            "Epoch 19/21 Iteration 40/49 Loss 0.38529592752456665: \n",
            "Epoch 19/21 Iteration 41/49 Loss 0.46438539028167725: \n",
            "Epoch 19/21 Iteration 42/49 Loss 0.43681126832962036: \n",
            "Epoch 19/21 Iteration 43/49 Loss 0.4018021523952484: \n",
            "Epoch 19/21 Iteration 44/49 Loss 0.46556931734085083: \n",
            "Epoch 19/21 Iteration 45/49 Loss 0.47917014360427856: \n",
            "Epoch 19/21 Iteration 46/49 Loss 0.49205517768859863: \n",
            "Epoch 19/21 Iteration 47/49 Loss 0.4322514533996582: \n",
            "Epoch 19/21 Iteration 48/49 Loss 0.4388737678527832: \n",
            "Epoch 20/21 Iteration 0/49 Loss 0.4483225345611572: \n",
            "Epoch 20/21 Iteration 1/49 Loss 0.46727296710014343: \n",
            "Epoch 20/21 Iteration 2/49 Loss 0.43147027492523193: \n",
            "Epoch 20/21 Iteration 3/49 Loss 0.4582316279411316: \n",
            "Epoch 20/21 Iteration 4/49 Loss 0.47823259234428406: \n",
            "Epoch 20/21 Iteration 5/49 Loss 0.47965800762176514: \n",
            "Epoch 20/21 Iteration 6/49 Loss 0.42746788263320923: \n",
            "Epoch 20/21 Iteration 7/49 Loss 0.4502257704734802: \n",
            "Epoch 20/21 Iteration 8/49 Loss 0.41573527455329895: \n",
            "Epoch 20/21 Iteration 9/49 Loss 0.4409981667995453: \n",
            "Epoch 20/21 Iteration 10/49 Loss 0.43947887420654297: \n",
            "Epoch 20/21 Iteration 11/49 Loss 0.41690242290496826: \n",
            "Epoch 20/21 Iteration 12/49 Loss 0.42264318466186523: \n",
            "Epoch 20/21 Iteration 13/49 Loss 0.41634610295295715: \n",
            "Epoch 20/21 Iteration 14/49 Loss 0.44040244817733765: \n",
            "Epoch 20/21 Iteration 15/49 Loss 0.43200090527534485: \n",
            "Epoch 20/21 Iteration 16/49 Loss 0.4727238118648529: \n",
            "Epoch 20/21 Iteration 17/49 Loss 0.43335801362991333: \n",
            "Epoch 20/21 Iteration 18/49 Loss 0.4573214650154114: \n",
            "Epoch 20/21 Iteration 19/49 Loss 0.4822944402694702: \n",
            "Epoch 20/21 Iteration 20/49 Loss 0.5279761552810669: \n",
            "Epoch 20/21 Iteration 21/49 Loss 0.4110061526298523: \n",
            "Epoch 20/21 Iteration 22/49 Loss 0.470257043838501: \n",
            "Epoch 20/21 Iteration 23/49 Loss 0.4357959032058716: \n",
            "Epoch 20/21 Iteration 24/49 Loss 0.5257028341293335: \n",
            "Epoch 20/21 Iteration 25/49 Loss 0.4586462378501892: \n",
            "Epoch 20/21 Iteration 26/49 Loss 0.4379696846008301: \n",
            "Epoch 20/21 Iteration 27/49 Loss 0.4481657147407532: \n",
            "Epoch 20/21 Iteration 28/49 Loss 0.40554606914520264: \n",
            "Epoch 20/21 Iteration 29/49 Loss 0.4683483839035034: \n",
            "Epoch 20/21 Iteration 30/49 Loss 0.43166518211364746: \n",
            "Epoch 20/21 Iteration 31/49 Loss 0.38885438442230225: \n",
            "Epoch 20/21 Iteration 32/49 Loss 0.4822462499141693: \n",
            "Epoch 20/21 Iteration 33/49 Loss 0.4734879434108734: \n",
            "Epoch 20/21 Iteration 34/49 Loss 0.43885964155197144: \n",
            "Epoch 20/21 Iteration 35/49 Loss 0.4251437187194824: \n",
            "Epoch 20/21 Iteration 36/49 Loss 0.40014657378196716: \n",
            "Epoch 20/21 Iteration 37/49 Loss 0.4663968086242676: \n",
            "Epoch 20/21 Iteration 38/49 Loss 0.41630446910858154: \n",
            "Epoch 20/21 Iteration 39/49 Loss 0.4410185217857361: \n",
            "Epoch 20/21 Iteration 40/49 Loss 0.38119596242904663: \n",
            "Epoch 20/21 Iteration 41/49 Loss 0.46096986532211304: \n",
            "Epoch 20/21 Iteration 42/49 Loss 0.43266561627388: \n",
            "Epoch 20/21 Iteration 43/49 Loss 0.3976631760597229: \n",
            "Epoch 20/21 Iteration 44/49 Loss 0.46147167682647705: \n",
            "Epoch 20/21 Iteration 45/49 Loss 0.4751378297805786: \n",
            "Epoch 20/21 Iteration 46/49 Loss 0.48788169026374817: \n",
            "Epoch 20/21 Iteration 47/49 Loss 0.4283825755119324: \n",
            "Epoch 20/21 Iteration 48/49 Loss 0.4340020716190338: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create new models here with different number of parameters, discuss and demonstrate underfitting and overfitting in NN\n",
        "#----------------------------------------------------\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 1024\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "class UnderFittingModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UnderFittingModel, self).__init__()\n",
        "    # Manipulaing 2 layers for underfitting but reducing the number of nodes\n",
        "    self.layer_1 = CustomLinearLayer(784, 80)\n",
        "    self.layer_2 = CustomLinearLayer(80, 10)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output_1 = self.layer_1(x)\n",
        "    output_1_actv = self.activation(output_1)\n",
        "    output_2 = self.layer_2(output_1_actv)\n",
        "    output = self.softmax(output_2)\n",
        "    return output\n",
        "\n",
        "modelUnderFitting = UnderFittingModel().cuda()\n",
        "ce_loss = CrossEntropyLoss()\n",
        "\n",
        "train_iter_loss =[]\n",
        "test_iter_loss = []\n",
        "training_loss = []\n",
        "testing_loss = []\n",
        "# Reducing the number of epoch for the training\n",
        "max_epochs = 5\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  for idx, data in enumerate(train_loader):\n",
        "    features, labels = data\n",
        "    features = features.cuda()\n",
        "\n",
        "    labels = labels.cuda()\n",
        "    probs = modelUnderFitting(features.reshape([-1, 784]))\n",
        "\n",
        "    loss = ce_loss(probs, labels)\n",
        "    train_iter_loss.append(loss.item())\n",
        "    print(\"Epoch {0}/{1} Iteration {2}/{3} Loss {4}: \".format(epoch, max_epochs, idx, len(train_loader), loss))\n",
        "\n",
        "    for param in modelUnderFitting.parameters():\n",
        "      param.grad = None\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in modelUnderFitting.named_parameters():\n",
        "      # Write paramtere update routine here:\n",
        "      # --------------\n",
        "      new_param = param - learning_rate * param.grad\n",
        "\n",
        "      # --------------\n",
        "      with torch.no_grad():\n",
        "        param.copy_(new_param)\n",
        "  training_loss.append(torch.mean(torch.Tensor(train_iter_loss)))\n",
        "\n",
        "  total = 0\n",
        "  accuracy_total = 0\n",
        "  recall_total = []\n",
        "  precision_total = []\n",
        "  f1_total = []\n",
        "  roc_auc_total = []\n",
        "\n",
        "  # Write Testing routine here:\n",
        "  #----------------------------\n",
        "  for idx, data in enumerate(test_loader):\n",
        "    features, labels = data\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      probs = modelOverFitting(features.reshape([-1, 784]))\n",
        "      loss = ce_loss(probs, labels)\n",
        "      test_iter_loss.append(loss.item())\n",
        "\n",
        "  testing_loss.append(torch.mean(torch.Tensor(test_iter_loss)))\n",
        "plt.plot(training_loss, label='Training Loss')\n",
        "plt.plot(testing_loss, label='Testing Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1210cea1-f728-4244-bb60-19623328a5c6",
        "id": "KaXnMtJ1c3DF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5 Iteration 0/49 Loss 2.685854434967041: \n",
            "Epoch 0/5 Iteration 1/49 Loss 2.5753092765808105: \n",
            "Epoch 0/5 Iteration 2/49 Loss 2.5097265243530273: \n",
            "Epoch 0/5 Iteration 3/49 Loss 2.472240447998047: \n",
            "Epoch 0/5 Iteration 4/49 Loss 2.4664745330810547: \n",
            "Epoch 0/5 Iteration 5/49 Loss 2.432116985321045: \n",
            "Epoch 0/5 Iteration 6/49 Loss 2.40918231010437: \n",
            "Epoch 0/5 Iteration 7/49 Loss 2.363949775695801: \n",
            "Epoch 0/5 Iteration 8/49 Loss 2.2961232662200928: \n",
            "Epoch 0/5 Iteration 9/49 Loss 2.307851552963257: \n",
            "Epoch 0/5 Iteration 10/49 Loss 2.3118553161621094: \n",
            "Epoch 0/5 Iteration 11/49 Loss 2.25954008102417: \n",
            "Epoch 0/5 Iteration 12/49 Loss 2.250519275665283: \n",
            "Epoch 0/5 Iteration 13/49 Loss 2.2687673568725586: \n",
            "Epoch 0/5 Iteration 14/49 Loss 2.2444419860839844: \n",
            "Epoch 0/5 Iteration 15/49 Loss 2.193808078765869: \n",
            "Epoch 0/5 Iteration 16/49 Loss 2.1962947845458984: \n",
            "Epoch 0/5 Iteration 17/49 Loss 2.183842897415161: \n",
            "Epoch 0/5 Iteration 18/49 Loss 2.1745553016662598: \n",
            "Epoch 0/5 Iteration 19/49 Loss 2.136399745941162: \n",
            "Epoch 0/5 Iteration 20/49 Loss 2.1525676250457764: \n",
            "Epoch 0/5 Iteration 21/49 Loss 2.1076340675354004: \n",
            "Epoch 0/5 Iteration 22/49 Loss 2.1359477043151855: \n",
            "Epoch 0/5 Iteration 23/49 Loss 2.1074156761169434: \n",
            "Epoch 0/5 Iteration 24/49 Loss 2.098466396331787: \n",
            "Epoch 0/5 Iteration 25/49 Loss 2.0818467140197754: \n",
            "Epoch 0/5 Iteration 26/49 Loss 2.0763468742370605: \n",
            "Epoch 0/5 Iteration 27/49 Loss 2.047740936279297: \n",
            "Epoch 0/5 Iteration 28/49 Loss 2.0491585731506348: \n",
            "Epoch 0/5 Iteration 29/49 Loss 2.0541720390319824: \n",
            "Epoch 0/5 Iteration 30/49 Loss 2.0335869789123535: \n",
            "Epoch 0/5 Iteration 31/49 Loss 2.02278733253479: \n",
            "Epoch 0/5 Iteration 32/49 Loss 2.003626823425293: \n",
            "Epoch 0/5 Iteration 33/49 Loss 1.9749882221221924: \n",
            "Epoch 0/5 Iteration 34/49 Loss 1.9672770500183105: \n",
            "Epoch 0/5 Iteration 35/49 Loss 1.985980749130249: \n",
            "Epoch 0/5 Iteration 36/49 Loss 1.9560718536376953: \n",
            "Epoch 0/5 Iteration 37/49 Loss 1.9522309303283691: \n",
            "Epoch 0/5 Iteration 38/49 Loss 1.9362170696258545: \n",
            "Epoch 0/5 Iteration 39/49 Loss 1.928493857383728: \n",
            "Epoch 0/5 Iteration 40/49 Loss 1.909275770187378: \n",
            "Epoch 0/5 Iteration 41/49 Loss 1.8825798034667969: \n",
            "Epoch 0/5 Iteration 42/49 Loss 1.8999156951904297: \n",
            "Epoch 0/5 Iteration 43/49 Loss 1.8872193098068237: \n",
            "Epoch 0/5 Iteration 44/49 Loss 1.9045865535736084: \n",
            "Epoch 0/5 Iteration 45/49 Loss 1.883375883102417: \n",
            "Epoch 0/5 Iteration 46/49 Loss 1.8688952922821045: \n",
            "Epoch 0/5 Iteration 47/49 Loss 1.8647518157958984: \n",
            "Epoch 0/5 Iteration 48/49 Loss 1.8369183540344238: \n",
            "Epoch 1/5 Iteration 0/49 Loss 1.836929440498352: \n",
            "Epoch 1/5 Iteration 1/49 Loss 1.8224046230316162: \n",
            "Epoch 1/5 Iteration 2/49 Loss 1.8061801195144653: \n",
            "Epoch 1/5 Iteration 3/49 Loss 1.7980250120162964: \n",
            "Epoch 1/5 Iteration 4/49 Loss 1.7992489337921143: \n",
            "Epoch 1/5 Iteration 5/49 Loss 1.8121439218521118: \n",
            "Epoch 1/5 Iteration 6/49 Loss 1.7923762798309326: \n",
            "Epoch 1/5 Iteration 7/49 Loss 1.7509269714355469: \n",
            "Epoch 1/5 Iteration 8/49 Loss 1.749626874923706: \n",
            "Epoch 1/5 Iteration 9/49 Loss 1.78400719165802: \n",
            "Epoch 1/5 Iteration 10/49 Loss 1.7622697353363037: \n",
            "Epoch 1/5 Iteration 11/49 Loss 1.7399301528930664: \n",
            "Epoch 1/5 Iteration 12/49 Loss 1.7309072017669678: \n",
            "Epoch 1/5 Iteration 13/49 Loss 1.7569465637207031: \n",
            "Epoch 1/5 Iteration 14/49 Loss 1.7457127571105957: \n",
            "Epoch 1/5 Iteration 15/49 Loss 1.6929742097854614: \n",
            "Epoch 1/5 Iteration 16/49 Loss 1.726617693901062: \n",
            "Epoch 1/5 Iteration 17/49 Loss 1.7065298557281494: \n",
            "Epoch 1/5 Iteration 18/49 Loss 1.7022106647491455: \n",
            "Epoch 1/5 Iteration 19/49 Loss 1.6837177276611328: \n",
            "Epoch 1/5 Iteration 20/49 Loss 1.7084192037582397: \n",
            "Epoch 1/5 Iteration 21/49 Loss 1.6675398349761963: \n",
            "Epoch 1/5 Iteration 22/49 Loss 1.6854958534240723: \n",
            "Epoch 1/5 Iteration 23/49 Loss 1.6698371171951294: \n",
            "Epoch 1/5 Iteration 24/49 Loss 1.6819957494735718: \n",
            "Epoch 1/5 Iteration 25/49 Loss 1.6486419439315796: \n",
            "Epoch 1/5 Iteration 26/49 Loss 1.643296241760254: \n",
            "Epoch 1/5 Iteration 27/49 Loss 1.6336894035339355: \n",
            "Epoch 1/5 Iteration 28/49 Loss 1.6324783563613892: \n",
            "Epoch 1/5 Iteration 29/49 Loss 1.659074306488037: \n",
            "Epoch 1/5 Iteration 30/49 Loss 1.6261324882507324: \n",
            "Epoch 1/5 Iteration 31/49 Loss 1.6141982078552246: \n",
            "Epoch 1/5 Iteration 32/49 Loss 1.603865146636963: \n",
            "Epoch 1/5 Iteration 33/49 Loss 1.5901944637298584: \n",
            "Epoch 1/5 Iteration 34/49 Loss 1.5767216682434082: \n",
            "Epoch 1/5 Iteration 35/49 Loss 1.5938645601272583: \n",
            "Epoch 1/5 Iteration 36/49 Loss 1.5615196228027344: \n",
            "Epoch 1/5 Iteration 37/49 Loss 1.571800708770752: \n",
            "Epoch 1/5 Iteration 38/49 Loss 1.5589079856872559: \n",
            "Epoch 1/5 Iteration 39/49 Loss 1.5540719032287598: \n",
            "Epoch 1/5 Iteration 40/49 Loss 1.531585931777954: \n",
            "Epoch 1/5 Iteration 41/49 Loss 1.5074952840805054: \n",
            "Epoch 1/5 Iteration 42/49 Loss 1.526296854019165: \n",
            "Epoch 1/5 Iteration 43/49 Loss 1.5317118167877197: \n",
            "Epoch 1/5 Iteration 44/49 Loss 1.5444916486740112: \n",
            "Epoch 1/5 Iteration 45/49 Loss 1.5480046272277832: \n",
            "Epoch 1/5 Iteration 46/49 Loss 1.5308455228805542: \n",
            "Epoch 1/5 Iteration 47/49 Loss 1.507034420967102: \n",
            "Epoch 1/5 Iteration 48/49 Loss 1.4944185018539429: \n",
            "Epoch 2/5 Iteration 0/49 Loss 1.4957101345062256: \n",
            "Epoch 2/5 Iteration 1/49 Loss 1.4833698272705078: \n",
            "Epoch 2/5 Iteration 2/49 Loss 1.4695615768432617: \n",
            "Epoch 2/5 Iteration 3/49 Loss 1.4715521335601807: \n",
            "Epoch 2/5 Iteration 4/49 Loss 1.4739868640899658: \n",
            "Epoch 2/5 Iteration 5/49 Loss 1.486539602279663: \n",
            "Epoch 2/5 Iteration 6/49 Loss 1.4666948318481445: \n",
            "Epoch 2/5 Iteration 7/49 Loss 1.430912971496582: \n",
            "Epoch 2/5 Iteration 8/49 Loss 1.4414925575256348: \n",
            "Epoch 2/5 Iteration 9/49 Loss 1.4781808853149414: \n",
            "Epoch 2/5 Iteration 10/49 Loss 1.455944538116455: \n",
            "Epoch 2/5 Iteration 11/49 Loss 1.4230550527572632: \n",
            "Epoch 2/5 Iteration 12/49 Loss 1.4260351657867432: \n",
            "Epoch 2/5 Iteration 13/49 Loss 1.454544186592102: \n",
            "Epoch 2/5 Iteration 14/49 Loss 1.4418965578079224: \n",
            "Epoch 2/5 Iteration 15/49 Loss 1.3908452987670898: \n",
            "Epoch 2/5 Iteration 16/49 Loss 1.4334475994110107: \n",
            "Epoch 2/5 Iteration 17/49 Loss 1.4115149974822998: \n",
            "Epoch 2/5 Iteration 18/49 Loss 1.4150394201278687: \n",
            "Epoch 2/5 Iteration 19/49 Loss 1.3963983058929443: \n",
            "Epoch 2/5 Iteration 20/49 Loss 1.425201177597046: \n",
            "Epoch 2/5 Iteration 21/49 Loss 1.3857252597808838: \n",
            "Epoch 2/5 Iteration 22/49 Loss 1.3981757164001465: \n",
            "Epoch 2/5 Iteration 23/49 Loss 1.391718864440918: \n",
            "Epoch 2/5 Iteration 24/49 Loss 1.4147276878356934: \n",
            "Epoch 2/5 Iteration 25/49 Loss 1.3717032670974731: \n",
            "Epoch 2/5 Iteration 26/49 Loss 1.3656010627746582: \n",
            "Epoch 2/5 Iteration 27/49 Loss 1.3651800155639648: \n",
            "Epoch 2/5 Iteration 28/49 Loss 1.3628884553909302: \n",
            "Epoch 2/5 Iteration 29/49 Loss 1.4030110836029053: \n",
            "Epoch 2/5 Iteration 30/49 Loss 1.363356351852417: \n",
            "Epoch 2/5 Iteration 31/49 Loss 1.3508563041687012: \n",
            "Epoch 2/5 Iteration 32/49 Loss 1.3442028760910034: \n",
            "Epoch 2/5 Iteration 33/49 Loss 1.3422768115997314: \n",
            "Epoch 2/5 Iteration 34/49 Loss 1.3252549171447754: \n",
            "Epoch 2/5 Iteration 35/49 Loss 1.3399430513381958: \n",
            "Epoch 2/5 Iteration 36/49 Loss 1.3075283765792847: \n",
            "Epoch 2/5 Iteration 37/49 Loss 1.3304293155670166: \n",
            "Epoch 2/5 Iteration 38/49 Loss 1.314894676208496: \n",
            "Epoch 2/5 Iteration 39/49 Loss 1.3102388381958008: \n",
            "Epoch 2/5 Iteration 40/49 Loss 1.2902761697769165: \n",
            "Epoch 2/5 Iteration 41/49 Loss 1.266968846321106: \n",
            "Epoch 2/5 Iteration 42/49 Loss 1.2856645584106445: \n",
            "Epoch 2/5 Iteration 43/49 Loss 1.298985242843628: \n",
            "Epoch 2/5 Iteration 44/49 Loss 1.3098708391189575: \n",
            "Epoch 2/5 Iteration 45/49 Loss 1.3289852142333984: \n",
            "Epoch 2/5 Iteration 46/49 Loss 1.3107401132583618: \n",
            "Epoch 2/5 Iteration 47/49 Loss 1.2746376991271973: \n",
            "Epoch 2/5 Iteration 48/49 Loss 1.2754182815551758: \n",
            "Epoch 3/5 Iteration 0/49 Loss 1.2771743535995483: \n",
            "Epoch 3/5 Iteration 1/49 Loss 1.2646061182022095: \n",
            "Epoch 3/5 Iteration 2/49 Loss 1.2534475326538086: \n",
            "Epoch 3/5 Iteration 3/49 Loss 1.2598307132720947: \n",
            "Epoch 3/5 Iteration 4/49 Loss 1.2637388706207275: \n",
            "Epoch 3/5 Iteration 5/49 Loss 1.27777099609375: \n",
            "Epoch 3/5 Iteration 6/49 Loss 1.255627155303955: \n",
            "Epoch 3/5 Iteration 7/49 Loss 1.225290060043335: \n",
            "Epoch 3/5 Iteration 8/49 Loss 1.2436590194702148: \n",
            "Epoch 3/5 Iteration 9/49 Loss 1.2767581939697266: \n",
            "Epoch 3/5 Iteration 10/49 Loss 1.2568550109863281: \n",
            "Epoch 3/5 Iteration 11/49 Loss 1.2166528701782227: \n",
            "Epoch 3/5 Iteration 12/49 Loss 1.22975492477417: \n",
            "Epoch 3/5 Iteration 13/49 Loss 1.2596547603607178: \n",
            "Epoch 3/5 Iteration 14/49 Loss 1.2433089017868042: \n",
            "Epoch 3/5 Iteration 15/49 Loss 1.196524977684021: \n",
            "Epoch 3/5 Iteration 16/49 Loss 1.2420830726623535: \n",
            "Epoch 3/5 Iteration 17/49 Loss 1.22042715549469: \n",
            "Epoch 3/5 Iteration 18/49 Loss 1.2313541173934937: \n",
            "Epoch 3/5 Iteration 19/49 Loss 1.2088475227355957: \n",
            "Epoch 3/5 Iteration 20/49 Loss 1.240849256515503: \n",
            "Epoch 3/5 Iteration 21/49 Loss 1.2015862464904785: \n",
            "Epoch 3/5 Iteration 22/49 Loss 1.2117540836334229: \n",
            "Epoch 3/5 Iteration 23/49 Loss 1.210456371307373: \n",
            "Epoch 3/5 Iteration 24/49 Loss 1.2404824495315552: \n",
            "Epoch 3/5 Iteration 25/49 Loss 1.1924289464950562: \n",
            "Epoch 3/5 Iteration 26/49 Loss 1.1859335899353027: \n",
            "Epoch 3/5 Iteration 27/49 Loss 1.1908485889434814: \n",
            "Epoch 3/5 Iteration 28/49 Loss 1.187208652496338: \n",
            "Epoch 3/5 Iteration 29/49 Loss 1.2351853847503662: \n",
            "Epoch 3/5 Iteration 30/49 Loss 1.1906768083572388: \n",
            "Epoch 3/5 Iteration 31/49 Loss 1.1799041032791138: \n",
            "Epoch 3/5 Iteration 32/49 Loss 1.1746286153793335: \n",
            "Epoch 3/5 Iteration 33/49 Loss 1.181779384613037: \n",
            "Epoch 3/5 Iteration 34/49 Loss 1.1618432998657227: \n",
            "Epoch 3/5 Iteration 35/49 Loss 1.1732131242752075: \n",
            "Epoch 3/5 Iteration 36/49 Loss 1.1423649787902832: \n",
            "Epoch 3/5 Iteration 37/49 Loss 1.17452871799469: \n",
            "Epoch 3/5 Iteration 38/49 Loss 1.155929446220398: \n",
            "Epoch 3/5 Iteration 39/49 Loss 1.150392770767212: \n",
            "Epoch 3/5 Iteration 40/49 Loss 1.1333444118499756: \n",
            "Epoch 3/5 Iteration 41/49 Loss 1.1100231409072876: \n",
            "Epoch 3/5 Iteration 42/49 Loss 1.1302871704101562: \n",
            "Epoch 3/5 Iteration 43/49 Loss 1.1451222896575928: \n",
            "Epoch 3/5 Iteration 44/49 Loss 1.1567203998565674: \n",
            "Epoch 3/5 Iteration 45/49 Loss 1.1853150129318237: \n",
            "Epoch 3/5 Iteration 46/49 Loss 1.166757345199585: \n",
            "Epoch 3/5 Iteration 47/49 Loss 1.1231257915496826: \n",
            "Epoch 3/5 Iteration 48/49 Loss 1.1336421966552734: \n",
            "Epoch 4/5 Iteration 0/49 Loss 1.1355470418930054: \n",
            "Epoch 4/5 Iteration 1/49 Loss 1.1216576099395752: \n",
            "Epoch 4/5 Iteration 2/49 Loss 1.1119627952575684: \n",
            "Epoch 4/5 Iteration 3/49 Loss 1.1210362911224365: \n",
            "Epoch 4/5 Iteration 4/49 Loss 1.1260085105895996: \n",
            "Epoch 4/5 Iteration 5/49 Loss 1.1420738697052002: \n",
            "Epoch 4/5 Iteration 6/49 Loss 1.1165051460266113: \n",
            "Epoch 4/5 Iteration 7/49 Loss 1.0908443927764893: \n",
            "Epoch 4/5 Iteration 8/49 Loss 1.1148899793624878: \n",
            "Epoch 4/5 Iteration 9/49 Loss 1.1428911685943604: \n",
            "Epoch 4/5 Iteration 10/49 Loss 1.1249754428863525: \n",
            "Epoch 4/5 Iteration 11/49 Loss 1.080694317817688: \n",
            "Epoch 4/5 Iteration 12/49 Loss 1.1009495258331299: \n",
            "Epoch 4/5 Iteration 13/49 Loss 1.1313071250915527: \n",
            "Epoch 4/5 Iteration 14/49 Loss 1.1114422082901: \n",
            "Epoch 4/5 Iteration 15/49 Loss 1.0677952766418457: \n",
            "Epoch 4/5 Iteration 16/49 Loss 1.1148152351379395: \n",
            "Epoch 4/5 Iteration 17/49 Loss 1.0938916206359863: \n",
            "Epoch 4/5 Iteration 18/49 Loss 1.1106064319610596: \n",
            "Epoch 4/5 Iteration 19/49 Loss 1.0842549800872803: \n",
            "Epoch 4/5 Iteration 20/49 Loss 1.1188914775848389: \n",
            "Epoch 4/5 Iteration 21/49 Loss 1.0785877704620361: \n",
            "Epoch 4/5 Iteration 22/49 Loss 1.0883510112762451: \n",
            "Epoch 4/5 Iteration 23/49 Loss 1.0891567468643188: \n",
            "Epoch 4/5 Iteration 24/49 Loss 1.1241554021835327: \n",
            "Epoch 4/5 Iteration 25/49 Loss 1.0733931064605713: \n",
            "Epoch 4/5 Iteration 26/49 Loss 1.0666847229003906: \n",
            "Epoch 4/5 Iteration 27/49 Loss 1.0748038291931152: \n",
            "Epoch 4/5 Iteration 28/49 Loss 1.0693870782852173: \n",
            "Epoch 4/5 Iteration 29/49 Loss 1.122030258178711: \n",
            "Epoch 4/5 Iteration 30/49 Loss 1.074181079864502: \n",
            "Epoch 4/5 Iteration 31/49 Loss 1.0654480457305908: \n",
            "Epoch 4/5 Iteration 32/49 Loss 1.0608222484588623: \n",
            "Epoch 4/5 Iteration 33/49 Loss 1.0741987228393555: \n",
            "Epoch 4/5 Iteration 34/49 Loss 1.0520740747451782: \n",
            "Epoch 4/5 Iteration 35/49 Loss 1.0599956512451172: \n",
            "Epoch 4/5 Iteration 36/49 Loss 1.0310041904449463: \n",
            "Epoch 4/5 Iteration 37/49 Loss 1.0697779655456543: \n",
            "Epoch 4/5 Iteration 38/49 Loss 1.0486470460891724: \n",
            "Epoch 4/5 Iteration 39/49 Loss 1.0418827533721924: \n",
            "Epoch 4/5 Iteration 40/49 Loss 1.0274556875228882: \n",
            "Epoch 4/5 Iteration 41/49 Loss 1.0040769577026367: \n",
            "Epoch 4/5 Iteration 42/49 Loss 1.0257580280303955: \n",
            "Epoch 4/5 Iteration 43/49 Loss 1.0393301248550415: \n",
            "Epoch 4/5 Iteration 44/49 Loss 1.0528099536895752: \n",
            "Epoch 4/5 Iteration 45/49 Loss 1.087486743927002: \n",
            "Epoch 4/5 Iteration 46/49 Loss 1.0691015720367432: \n",
            "Epoch 4/5 Iteration 47/49 Loss 1.0204570293426514: \n",
            "Epoch 4/5 Iteration 48/49 Loss 1.037839651107788: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to identify overfitting by comparing the training loss to the testing loss. If the training loss is smaller than the testing loss, it can be assumed that the model has to overfit the training data and is not properly generalizing to new data. On the other hand, if both the training loss and the testing loss are higher than the equivalent values for a benchmark model, then suggests that the model is underfitting and has not been successful in learning the underlying patterns in the data.\n",
        "An study of the training loss and testing loss can be used to determine whether a model is overfitting or underfitting. The model has a high degree of fit to the training data, but is not generalizing well to new, unseen data, it can be concluded if the training loss is determined to be smaller than the testing loss. Overfitting is indicated by the given. On the other hand, if the training loss and testing loss are both higher than the comparable figures for a benchmark model, then indicates that the model is underfitting and is not effectively capturing the underlying patterns in the data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nyAA2kfyeQvL"
      }
    }
  ]
}